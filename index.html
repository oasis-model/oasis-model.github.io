<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/a34f9d1faa5f3315-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/8a84935111142e76.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-0954c5d36ad957a9.js"/><script src="/_next/static/chunks/fd9d1056-3f777ab9f9509526.js" async=""></script><script src="/_next/static/chunks/117-b4af6d3f17e58014.js" async=""></script><script src="/_next/static/chunks/main-app-eee36d936b6e2699.js" async=""></script><script src="/_next/static/chunks/0e5ce63c-a42bf838a066fc6f.js" async=""></script><script src="/_next/static/chunks/401-82f646aab98d244d.js" async=""></script><script src="/_next/static/chunks/app/page-b9eeb16764b91169.js" async=""></script><title>Oasis</title><meta name="description" content="Generating Worlds in Realtime"/><link rel="icon" href="/favicon.ico" type="image/x-icon" sizes="16x16"/><meta name="next-size-adjust"/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="__className_d65c78 antialiased"><div class=""><main class="flex flex-col gap-8  p-8 pb-20 gap-16 sm:p-20 md:px-48 "><div class="flex flex-col items-center gap-8"><h3>October 31, 2024</h3><h1 class="text-5xl font-bold text-center">Oasis: A Universe in a Transformer</h1><h2 class="mt-[-20px] text-center mb-8"><a class="underline" href="https://decart.ai">Decart</a>, <a class="underline" href="https://etched.com">Etched</a></h2><video src="/wide1.mp4" autoPlay="" loop="" muted="" playsInline="" class="w-full h-full object-cover"></video><div class="flex flex-col items-center justify-center gap-4 w-full md:w-1/2"><a class="rounded-full border border-solid border-transparent transition-colors flex items-center justify-center bg-foreground text-background gap-2 hover:bg-[#383838] dark:hover:bg-[#ccc] text-sm sm:text-base h-14 sm:h-16 px-4 sm:px-5 w-56" href="" target="_blank" rel="noopener noreferrer"><svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg" class="w-4 h-4"><path d="M3.24182 2.32181C3.3919 2.23132 3.5784 2.22601 3.73338 2.30781L12.7334 7.05781C12.8974 7.14436 13 7.31457 13 7.5C13 7.68543 12.8974 7.85564 12.7334 7.94219L3.73338 12.6922C3.5784 12.774 3.3919 12.7687 3.24182 12.6782C3.09175 12.5877 3 12.4252 3 12.25V2.75C3 2.57476 3.09175 2.4123 3.24182 2.32181ZM4 3.57925V11.4207L11.4288 7.5L4 3.57925Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg><b>Play demo</b></a></div><div class="flex gap-4 flex-col sm:flex-row w-full items-center justify-center mt-[-14px]"><a class="rounded-full border border-solid border-foreground transition-colors flex items-center justify-center gap-2 hover:bg-[#ccc] dark:hover:bg-[#383838] text-sm sm:text-base h-10 sm:h-12 px-4 sm:px-5" href="" target="_blank" rel="noopener noreferrer"><svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg" class="w-4 h-4"><path d="M3.64645 11.3536C3.45118 11.1583 3.45118 10.8417 3.64645 10.6465L10.2929 4L6 4C5.72386 4 5.5 3.77614 5.5 3.5C5.5 3.22386 5.72386 3 6 3L11.5 3C11.6326 3 11.7598 3.05268 11.8536 3.14645C11.9473 3.24022 12 3.36739 12 3.5L12 9.00001C12 9.27615 11.7761 9.50001 11.5 9.50001C11.2239 9.50001 11 9.27615 11 9.00001V4.70711L4.35355 11.3536C4.15829 11.5488 3.84171 11.5488 3.64645 11.3536Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg>Decart Blog</a><a class="rounded-full border border-solid border-foreground transition-colors flex items-center justify-center gap-2 hover:bg-[#ccc] dark:hover:bg-[#383838] text-sm sm:text-base h-10 sm:h-12 px-4 sm:px-5" href="" target="_blank" rel="noopener noreferrer"><img alt="GitHub" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="dark:invert" style="color:transparent" src="https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg"/>View code</a><a class="rounded-full border border-solid border-foreground transition-colors flex items-center justify-center gap-2 hover:bg-[#ccc] dark:hover:bg-[#383838] text-sm sm:text-base h-10 sm:h-12 px-4 sm:px-5" href="" target="https://etched.com/blog-posts/oasis" rel="noopener noreferrer"><svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg" class="w-4 h-4"><path d="M3.64645 11.3536C3.45118 11.1583 3.45118 10.8417 3.64645 10.6465L10.2929 4L6 4C5.72386 4 5.5 3.77614 5.5 3.5C5.5 3.22386 5.72386 3 6 3L11.5 3C11.6326 3 11.7598 3.05268 11.8536 3.14645C11.9473 3.24022 12 3.36739 12 3.5L12 9.00001C12 9.27615 11.7761 9.50001 11.5 9.50001C11.2239 9.50001 11 9.27615 11 9.00001V4.70711L4.35355 11.3536C4.15829 11.5488 3.84171 11.5488 3.64645 11.3536Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg>Etched Blog</a></div></div><div class="flex flex-col gap-4 px-0 lg:px-32"><p class="text-justify leading-relaxed">We&#x27;re excited to announce Oasis, the first playable, realtime, open-world AI model. It&#x27;s a video game, but entirely generated by AI. Oasis is the first step in our research towards more complex interactive worlds. </p><p class="text-justify leading-relaxed">Oasis takes in user keyboard input and generates real-time gameplay, including physics, game rules, and graphics. You can move around, jump, pick up items, break blocks, and more. There is no game engine; just a foundation model.</p><p class="text-justify leading-relaxed">We believe fast transformer inference is the missing link to making generative video a reality. Today, using Decart&#x27;s inference engine, we show that real-time video is possible. When Etched&#x27;s transformer ASIC, Sohu, is released, we can run models like Oasis in 4K. Today, we&#x27;re releasing Oasis&#x27;s code, the weights of a 500M parameter model you can run locally, and a live playable demo of a larger checkpoint.</p><h2 class="text-2xl font-bold text-center mt-8">Gameplay Results</h2><div class="relative w-screen left-1/2 right-1/2 -mx-[50vw] my-4"><div class="relative w-full" role="region" aria-roledescription="carousel"><div class="overflow-hidden"><div class="flex -ml-4"><div role="group" aria-roledescription="slide" class="min-w-0 shrink-0 grow-0 pl-4 basis-1/2 md:basis-1/3"><div class="flex flex-col items-center justify-center"><img alt="Placing non-cube blocks" loading="lazy" width="700" height="300" decoding="async" data-nimg="1" class="object-cover" style="color:transparent" src="/placing_4_fences.webp"/><p class="text-sm text-center p-2">Placing non-cube blocks</p></div></div><div role="group" aria-roledescription="slide" class="min-w-0 shrink-0 grow-0 pl-4 basis-1/2 md:basis-1/3"><div class="flex flex-col items-center justify-center"><img alt="Model understands lighting physics" loading="lazy" width="700" height="300" decoding="async" data-nimg="1" class="object-cover" style="color:transparent" src="/torch_becomes_dark.webp"/><p class="text-sm text-center p-2">Model understands lighting physics</p></div></div><div role="group" aria-roledescription="slide" class="min-w-0 shrink-0 grow-0 pl-4 basis-1/2 md:basis-1/3"><div class="flex flex-col items-center justify-center"><img alt="Real-time block manipulation" loading="lazy" width="700" height="300" decoding="async" data-nimg="1" class="object-cover" style="color:transparent" src="/inventory_and_changing_hands.webp"/><p class="text-sm text-center p-2">Real-time block manipulation</p></div></div><div role="group" aria-roledescription="slide" class="min-w-0 shrink-0 grow-0 pl-4 basis-1/2 md:basis-1/3"><div class="flex flex-col items-center justify-center"><img alt="Interactive crafting system" loading="lazy" width="700" height="300" decoding="async" data-nimg="1" class="object-cover" style="color:transparent" src="/2.webp"/><p class="text-sm text-center p-2">Interactive crafting system</p></div></div><div role="group" aria-roledescription="slide" class="min-w-0 shrink-0 grow-0 pl-4 basis-1/2 md:basis-1/3"><div class="flex flex-col items-center justify-center"><img alt="AI-driven NPCs and wildlife" loading="lazy" width="700" height="300" decoding="async" data-nimg="1" class="object-cover" style="color:transparent" src="/3.webp"/><p class="text-sm text-center p-2">AI-driven NPCs and wildlife</p></div></div></div></div><div class="flex justify-center gap-2 mt-4"><button class="inline-flex items-center justify-center gap-2 whitespace-nowrap text-sm font-medium transition-colors focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg]:size-4 [&amp;_svg]:shrink-0 border border-input bg-background shadow-sm hover:bg-accent hover:text-accent-foreground h-8 w-8 rounded-full -left-12 top-1/2 static translate-y-0" disabled=""><svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg" class="h-4 w-4"><path d="M6.85355 3.14645C7.04882 3.34171 7.04882 3.65829 6.85355 3.85355L3.70711 7H12.5C12.7761 7 13 7.22386 13 7.5C13 7.77614 12.7761 8 12.5 8H3.70711L6.85355 11.1464C7.04882 11.3417 7.04882 11.6583 6.85355 11.8536C6.65829 12.0488 6.34171 12.0488 6.14645 11.8536L2.14645 7.85355C1.95118 7.65829 1.95118 7.34171 2.14645 7.14645L6.14645 3.14645C6.34171 2.95118 6.65829 2.95118 6.85355 3.14645Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg><span class="sr-only">Previous slide</span></button><button class="inline-flex items-center justify-center gap-2 whitespace-nowrap text-sm font-medium transition-colors focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg]:size-4 [&amp;_svg]:shrink-0 border border-input bg-background shadow-sm hover:bg-accent hover:text-accent-foreground h-8 w-8 rounded-full -right-12 top-1/2 static translate-y-0" disabled=""><svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg" class="h-4 w-4"><path d="M8.14645 3.14645C8.34171 2.95118 8.65829 2.95118 8.85355 3.14645L12.8536 7.14645C13.0488 7.34171 13.0488 7.65829 12.8536 7.85355L8.85355 11.8536C8.65829 12.0488 8.34171 12.0488 8.14645 11.8536C7.95118 11.6583 7.95118 11.3417 8.14645 11.1464L11.2929 8H2.5C2.22386 8 2 7.77614 2 7.5C2 7.22386 2.22386 7 2.5 7H11.2929L8.14645 3.85355C7.95118 3.65829 7.95118 3.34171 8.14645 3.14645Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg><span class="sr-only">Next slide</span></button></div></div></div><p class="text-justify leading-relaxed">Our model even understands complex game mechanics, such as crafting, and NPCs ADD MORE EXAMPLES.</p><div class="relative w-screen left-1/2 right-1/2 -mx-[50vw] my-4"><div class="relative w-full" role="region" aria-roledescription="carousel"><div class="overflow-hidden"><div class="flex -ml-4"><div role="group" aria-roledescription="slide" class="min-w-0 shrink-0 grow-0 pl-4 basis-1/2 md:basis-1/3"><div class="rounded-xl border bg-card text-card-foreground shadow"><div class="flex items-center justify-center p-0"><img alt="Carousel image 1" loading="lazy" width="700" height="300" decoding="async" data-nimg="1" class="object-cover" style="color:transparent" src="/1.webp"/></div></div></div><div role="group" aria-roledescription="slide" class="min-w-0 shrink-0 grow-0 pl-4 basis-1/2 md:basis-1/3"><div class="rounded-xl border bg-card text-card-foreground shadow"><div class="flex items-center justify-center p-0"><img alt="Carousel image 2" loading="lazy" width="700" height="300" decoding="async" data-nimg="1" class="object-cover" style="color:transparent" src="/2.webp"/></div></div></div><div role="group" aria-roledescription="slide" class="min-w-0 shrink-0 grow-0 pl-4 basis-1/2 md:basis-1/3"><div class="rounded-xl border bg-card text-card-foreground shadow"><div class="flex items-center justify-center p-0"><img alt="Carousel image 3" loading="lazy" width="700" height="300" decoding="async" data-nimg="1" class="object-cover" style="color:transparent" src="/3.webp"/></div></div></div><div role="group" aria-roledescription="slide" class="min-w-0 shrink-0 grow-0 pl-4 basis-1/2 md:basis-1/3"><div class="rounded-xl border bg-card text-card-foreground shadow"><div class="flex items-center justify-center p-0"><img alt="Carousel image 4" loading="lazy" width="700" height="300" decoding="async" data-nimg="1" class="object-cover" style="color:transparent" src="/4.webp"/></div></div></div><div role="group" aria-roledescription="slide" class="min-w-0 shrink-0 grow-0 pl-4 basis-1/2 md:basis-1/3"><div class="rounded-xl border bg-card text-card-foreground shadow"><div class="flex items-center justify-center p-0"><img alt="Carousel image 5" loading="lazy" width="700" height="300" decoding="async" data-nimg="1" class="object-cover" style="color:transparent" src="/5.webp"/></div></div></div></div></div><div class="flex justify-center gap-2 mt-4"><button class="inline-flex items-center justify-center gap-2 whitespace-nowrap text-sm font-medium transition-colors focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg]:size-4 [&amp;_svg]:shrink-0 border border-input bg-background shadow-sm hover:bg-accent hover:text-accent-foreground h-8 w-8 rounded-full -left-12 top-1/2 static translate-y-0" disabled=""><svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg" class="h-4 w-4"><path d="M6.85355 3.14645C7.04882 3.34171 7.04882 3.65829 6.85355 3.85355L3.70711 7H12.5C12.7761 7 13 7.22386 13 7.5C13 7.77614 12.7761 8 12.5 8H3.70711L6.85355 11.1464C7.04882 11.3417 7.04882 11.6583 6.85355 11.8536C6.65829 12.0488 6.34171 12.0488 6.14645 11.8536L2.14645 7.85355C1.95118 7.65829 1.95118 7.34171 2.14645 7.14645L6.14645 3.14645C6.34171 2.95118 6.65829 2.95118 6.85355 3.14645Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg><span class="sr-only">Previous slide</span></button><button class="inline-flex items-center justify-center gap-2 whitespace-nowrap text-sm font-medium transition-colors focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg]:size-4 [&amp;_svg]:shrink-0 border border-input bg-background shadow-sm hover:bg-accent hover:text-accent-foreground h-8 w-8 rounded-full -right-12 top-1/2 static translate-y-0" disabled=""><svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg" class="h-4 w-4"><path d="M8.14645 3.14645C8.34171 2.95118 8.65829 2.95118 8.85355 3.14645L12.8536 7.14645C13.0488 7.34171 13.0488 7.65829 12.8536 7.85355L8.85355 11.8536C8.65829 12.0488 8.34171 12.0488 8.14645 11.8536C7.95118 11.6583 7.95118 11.3417 8.14645 11.1464L11.2929 8H2.5C2.22386 8 2 7.77614 2 7.5C2 7.22386 2.22386 7 2.5 7H11.2929L8.14645 3.85355C7.95118 3.65829 7.95118 3.34171 8.14645 3.14645Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg><span class="sr-only">Next slide</span></button></div></div></div><h2 class="text-2xl font-bold text-center mt-8">Architecture</h2><p class="text-justify leading-relaxed">The model is composed of two parts: a spatial autoencoder, and a latent diffusion backbone. Both are Transformer-based: the autoencoder is based on ViT<sup class="text-gray-500">[1]</sup>, and the backbone is based on DiT<sup class="text-gray-500">[2]</sup>. We chose Transformers to ensure stable, predictable scaling, and fast inference on Etched&#x27;s Transformer ASIC, Sohu.</p><img alt="Architecture" loading="lazy" width="1000" height="500" decoding="async" data-nimg="1" style="color:transparent" src="/arch_new.png"/><p class="text-justify leading-relaxed">In contrast to bidirectional models such as Sora<sup class="text-gray-500">[3]</sup>, Oasis generates frames autoregressively, with the ability to condition each frame on game input. This enables users to interact with the world in real-time. The model was trained using Diffusion Forcing<sup class="text-gray-500">[4]</sup>, which denoises with independent per-token noise levels, and allows for novel decoding schemes such as ours.</p><p class="text-justify leading-relaxed">One issue we focused on is temporal stability--making sure the model outputs make sense over long time horizons. In autoregressive models, errors compound, and small imperfections can quickly snowball into glitched frames. Solving this required innovations in long-context generation.</p><p class="text-justify leading-relaxed">We solved this by deploying dynamic noising, which adjusts inference-time noise on a schedule, injecting noise in the first diffusion forward passes to reduce error accumulation, and gradually removing noise in the later passes so the model can find and persist high-frequency details in previous frames for improved consistency. Since our model saw noise during training, it learned to successfully deal with noisy samples at inference.</p><div class="flex justify-center items-center"><img alt="Dynamic Noising" loading="lazy" width="700" height="500" decoding="async" data-nimg="1" style="color:transparent" src="/dyno.png"/></div><p class="text-justify leading-relaxed">To learn more about the engineering underlying this model, and some of the specific optimizations in training and inference, check out the <a class="underline" href="https://decart.ai/blog/training-the-world-model">Decart blog post</a>.</p><div class="relative w-screen left-1/2 right-1/2 -mx-[50vw] my-4"><div class="relative w-full" role="region" aria-roledescription="carousel"><div class="overflow-hidden"><div class="flex -ml-4"><div role="group" aria-roledescription="slide" class="min-w-0 shrink-0 grow-0 pl-4 basis-1/2 md:basis-1/3"><div class="rounded-xl border bg-card text-card-foreground shadow"><div class="flex items-center justify-center p-0"><img alt="Carousel image 1" loading="lazy" width="700" height="300" decoding="async" data-nimg="1" class="object-cover" style="color:transparent" src="/5.webp"/></div></div></div><div role="group" aria-roledescription="slide" class="min-w-0 shrink-0 grow-0 pl-4 basis-1/2 md:basis-1/3"><div class="rounded-xl border bg-card text-card-foreground shadow"><div class="flex items-center justify-center p-0"><img alt="Carousel image 2" loading="lazy" width="700" height="300" decoding="async" data-nimg="1" class="object-cover" style="color:transparent" src="/4.webp"/></div></div></div><div role="group" aria-roledescription="slide" class="min-w-0 shrink-0 grow-0 pl-4 basis-1/2 md:basis-1/3"><div class="rounded-xl border bg-card text-card-foreground shadow"><div class="flex items-center justify-center p-0"><img alt="Carousel image 3" loading="lazy" width="700" height="300" decoding="async" data-nimg="1" class="object-cover" style="color:transparent" src="/1.webp"/></div></div></div><div role="group" aria-roledescription="slide" class="min-w-0 shrink-0 grow-0 pl-4 basis-1/2 md:basis-1/3"><div class="rounded-xl border bg-card text-card-foreground shadow"><div class="flex items-center justify-center p-0"><img alt="Carousel image 4" loading="lazy" width="700" height="300" decoding="async" data-nimg="1" class="object-cover" style="color:transparent" src="/2.webp"/></div></div></div><div role="group" aria-roledescription="slide" class="min-w-0 shrink-0 grow-0 pl-4 basis-1/2 md:basis-1/3"><div class="rounded-xl border bg-card text-card-foreground shadow"><div class="flex items-center justify-center p-0"><img alt="Carousel image 5" loading="lazy" width="700" height="300" decoding="async" data-nimg="1" class="object-cover" style="color:transparent" src="/3.webp"/></div></div></div></div></div><div class="flex justify-center gap-2 mt-4"><button class="inline-flex items-center justify-center gap-2 whitespace-nowrap text-sm font-medium transition-colors focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg]:size-4 [&amp;_svg]:shrink-0 border border-input bg-background shadow-sm hover:bg-accent hover:text-accent-foreground h-8 w-8 rounded-full -left-12 top-1/2 static translate-y-0" disabled=""><svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg" class="h-4 w-4"><path d="M6.85355 3.14645C7.04882 3.34171 7.04882 3.65829 6.85355 3.85355L3.70711 7H12.5C12.7761 7 13 7.22386 13 7.5C13 7.77614 12.7761 8 12.5 8H3.70711L6.85355 11.1464C7.04882 11.3417 7.04882 11.6583 6.85355 11.8536C6.65829 12.0488 6.34171 12.0488 6.14645 11.8536L2.14645 7.85355C1.95118 7.65829 1.95118 7.34171 2.14645 7.14645L6.14645 3.14645C6.34171 2.95118 6.65829 2.95118 6.85355 3.14645Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg><span class="sr-only">Previous slide</span></button><button class="inline-flex items-center justify-center gap-2 whitespace-nowrap text-sm font-medium transition-colors focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg]:size-4 [&amp;_svg]:shrink-0 border border-input bg-background shadow-sm hover:bg-accent hover:text-accent-foreground h-8 w-8 rounded-full -right-12 top-1/2 static translate-y-0" disabled=""><svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg" class="h-4 w-4"><path d="M8.14645 3.14645C8.34171 2.95118 8.65829 2.95118 8.85355 3.14645L12.8536 7.14645C13.0488 7.34171 13.0488 7.65829 12.8536 7.85355L8.85355 11.8536C8.65829 12.0488 8.34171 12.0488 8.14645 11.8536C7.95118 11.6583 7.95118 11.3417 8.14645 11.1464L11.2929 8H2.5C2.22386 8 2 7.77614 2 7.5C2 7.22386 2.22386 7 2.5 7H11.2929L8.14645 3.85355C7.95118 3.65829 7.95118 3.34171 8.14645 3.14645Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg><span class="sr-only">Next slide</span></button></div></div></div><h2 class="text-2xl font-bold text-center mt-8">Performance</h2><p class="text-justify leading-relaxed">Oasis generates real-time output in 24 frames per second. Current state-of-the-art text-to-video models with a similar DiT architecture (e.g. Sora<sup class="text-gray-500">[3]</sup>, Mochi-1<sup class="text-gray-500">[6]</sup> and Runway<sup class="text-gray-500">[7]</sup>) can take 10-20 seconds to create just one second of video, even on multiple GPUs. In order to match the experience of playing a game, however, our model must generate a new frame every 0.04 seconds, which is over 100x faster.</p><section class="flex justify-center items-center my-4"><img alt="Performance" loading="lazy" width="500" height="500" decoding="async" data-nimg="1" style="color:transparent" src="/speed.png"/></section><p class="text-justify leading-relaxed">With Decart&#x27;s inference stack, the model runs at playable framerates, unlocking real-time interactivity for the first time. Read more about it on <a class="underline" href="https://decart.ai/blog/training-the-world-model">Decart&#x27;s blog</a>.</p><p class="text-justify leading-relaxed">However, to make the model an additional order of magnitude faster, and make it cost-efficient to run at scale, new hardware is needed. Oasis is optimized for Sohu, the Transformer ASIC built by Etched. On NVIDIA H100s today, the model can run at 720p at 20fps. Sohu can run the same model at up to 4K, which is 4x more tokens per second. </p><p class="text-justify leading-relaxed">In addition, Oasis&#x27; end-to-end Transformer architecture makes it extremely efficient on Sohu - at the same price and power consumption as an H100 GPU, Oasis on Sohu can serve 10x more users. We believe the price of serving models like Oasis is the hidden bottleneck to releasing generative video in production. See more performance figures and read more about Oasis and Sohu on <a class="underline" href="https://etched.ai/blog">Etched&#x27;s blog</a>.</p><section class="flex justify-center items-center my-4"><img alt="Performance" loading="lazy" width="500" height="500" decoding="async" data-nimg="1" style="color:transparent" src="/sohu.png"/></section><h2 class="text-2xl font-bold text-center mt-8">Future Explorations</h2><p class="text-justify leading-relaxed"> With the many exciting results, there come areas for future development in the model. There are difficulties with the sometimes fuzzy video in the distance, the temporal consistency of uncertain objects, domain generalization, and difficulties over long contexts.</p><div class="relative w-screen left-1/2 right-1/2 -mx-[50vw] my-4"><div class="relative w-full" role="region" aria-roledescription="carousel"><div class="overflow-hidden"><div class="flex -ml-4"><div role="group" aria-roledescription="slide" class="min-w-0 shrink-0 grow-0 pl-4 basis-1/2 md:basis-1/3"><div class="flex flex-col items-center justify-center"><img alt="Placing non-cube blocks" loading="lazy" width="700" height="300" decoding="async" data-nimg="1" class="object-cover" style="color:transparent" src="/placing_4_fences.webp"/><p class="text-sm text-center p-2">Placing non-cube blocks</p></div></div><div role="group" aria-roledescription="slide" class="min-w-0 shrink-0 grow-0 pl-4 basis-1/2 md:basis-1/3"><div class="flex flex-col items-center justify-center"><img alt="Model understands lighting physics" loading="lazy" width="700" height="300" decoding="async" data-nimg="1" class="object-cover" style="color:transparent" src="/torch_becomes_dark.webp"/><p class="text-sm text-center p-2">Model understands lighting physics</p></div></div><div role="group" aria-roledescription="slide" class="min-w-0 shrink-0 grow-0 pl-4 basis-1/2 md:basis-1/3"><div class="flex flex-col items-center justify-center"><img alt="Real-time block manipulation" loading="lazy" width="700" height="300" decoding="async" data-nimg="1" class="object-cover" style="color:transparent" src="/inventory_and_changing_hands.webp"/><p class="text-sm text-center p-2">Real-time block manipulation</p></div></div><div role="group" aria-roledescription="slide" class="min-w-0 shrink-0 grow-0 pl-4 basis-1/2 md:basis-1/3"><div class="flex flex-col items-center justify-center"><img alt="Interactive crafting system" loading="lazy" width="700" height="300" decoding="async" data-nimg="1" class="object-cover" style="color:transparent" src="/2.webp"/><p class="text-sm text-center p-2">Interactive crafting system</p></div></div><div role="group" aria-roledescription="slide" class="min-w-0 shrink-0 grow-0 pl-4 basis-1/2 md:basis-1/3"><div class="flex flex-col items-center justify-center"><img alt="AI-driven NPCs and wildlife" loading="lazy" width="700" height="300" decoding="async" data-nimg="1" class="object-cover" style="color:transparent" src="/3.webp"/><p class="text-sm text-center p-2">AI-driven NPCs and wildlife</p></div></div></div></div><div class="flex justify-center gap-2 mt-4"><button class="inline-flex items-center justify-center gap-2 whitespace-nowrap text-sm font-medium transition-colors focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg]:size-4 [&amp;_svg]:shrink-0 border border-input bg-background shadow-sm hover:bg-accent hover:text-accent-foreground h-8 w-8 rounded-full -left-12 top-1/2 static translate-y-0" disabled=""><svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg" class="h-4 w-4"><path d="M6.85355 3.14645C7.04882 3.34171 7.04882 3.65829 6.85355 3.85355L3.70711 7H12.5C12.7761 7 13 7.22386 13 7.5C13 7.77614 12.7761 8 12.5 8H3.70711L6.85355 11.1464C7.04882 11.3417 7.04882 11.6583 6.85355 11.8536C6.65829 12.0488 6.34171 12.0488 6.14645 11.8536L2.14645 7.85355C1.95118 7.65829 1.95118 7.34171 2.14645 7.14645L6.14645 3.14645C6.34171 2.95118 6.65829 2.95118 6.85355 3.14645Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg><span class="sr-only">Previous slide</span></button><button class="inline-flex items-center justify-center gap-2 whitespace-nowrap text-sm font-medium transition-colors focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg]:size-4 [&amp;_svg]:shrink-0 border border-input bg-background shadow-sm hover:bg-accent hover:text-accent-foreground h-8 w-8 rounded-full -right-12 top-1/2 static translate-y-0" disabled=""><svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg" class="h-4 w-4"><path d="M8.14645 3.14645C8.34171 2.95118 8.65829 2.95118 8.85355 3.14645L12.8536 7.14645C13.0488 7.34171 13.0488 7.65829 12.8536 7.85355L8.85355 11.8536C8.65829 12.0488 8.34171 12.0488 8.14645 11.8536C7.95118 11.6583 7.95118 11.3417 8.14645 11.1464L11.2929 8H2.5C2.22386 8 2 7.77614 2 7.5C2 7.22386 2.22386 7 2.5 7H11.2929L8.14645 3.85355C7.95118 3.65829 7.95118 3.34171 8.14645 3.14645Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg><span class="sr-only">Next slide</span></button></div></div></div><h2 class="text-2xl font-bold text-center mt-8">What&#x27;s Next</h2><p class="text-justify leading-relaxed">Oasis is an impressive technical demo, but we believe this research will enable an exciting new generation of foundation models and consumer products. For example:</p><ul class="list-disc list-inside"><li>Creating and editing game content on-the-fly, even while playing, through text and image prompting</li><li>Generating content individually tailored for each user on social media</li><li>Real-time medical care, able to respond on the fly to patients in a video call</li><li>AI teachers, who can generate videos to respond to students in a classroom</li></ul><p class="text-justify leading-relaxed">Oasis is the first in a series of world generation models - we&#x27;re scaling our dataset and architecture by 10x each, and we&#x27;re excited to report our findings soon.</p><p class="text-justify leading-relaxed">Etched and Decart are excited to build these models together. The integration of Oasis with our hardware and software, from architecture to production, will ensure this model family remains one of the fastest and best as we advance the frontier of world generation. If you&#x27;re interested in collaborating, reach out to <a href="mailto:tal@decart.ai">tal@decart.ai</a></p></div></main><div class="flex flex-col gap-4 p-8 sm:p-20 md:px-48 bg-secondary"><p class="text-justify text-sm text-secondary-foreground">[1]: <a class="underline" href="https://arxiv.org/abs/2010.11929">Dosovitskiy et al., An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a></p><p class="text-justify text-sm text-secondary-foreground">[2]: <a class="underline" href="https://arxiv.org/abs/2212.09748">Peebles et al., Scalable Diffusion Models with Transformers</a></p><p class="text-justify text-sm text-secondary-foreground">[3]: <a class="underline" href="https://openai.com/index/video-generation-models-as-world-simulators/">OpenAI, Video generation models as world simulators</a></p><p class="text-justify text-sm text-secondary-foreground">[4]: <a class="underline" href="https://arxiv.org/abs/2407.01392">Chen et al., Diffusion Forcing: Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion</a></p><p class="text-justify text-sm text-secondary-foreground">[5]: <a class="underline" href="https://arxiv.org/abs/2211.05102">Pope et al., Efficiently Scaling Transformer Inference</a></p><p class="text-justify text-sm text-secondary-foreground">[6]: <a class="underline" href="https://www.genmo.ai/blog">Genmo, Mochi 1: A new SOTA in open-source video generation models</a></p><p class="text-justify text-sm text-secondary-foreground">[7]: <a class="underline" href="https://runwayml.com/research/introducing-gen-3-alpha">Runway, Introducing Gen-3 Alpha: A New Frontier for Video Generation</a></p><p class="text-justify text-sm text-secondary-foreground">* Estimated throughput figures - Sora reported, Mochi-1 from FAL.AI endpoint adjusted for parameter count, Runway from Gen-3 reported throughput</p></div><div class="flex flex-col gap-4 px-0 md:px-16 bg-black"><div class="flex flex-col gap-4 p-8 pb-20 gap-16 sm:p-20 md:px-48"><h2 class="text-2xl font-bold text-center text-white">Contributors</h2><p class="text-white text-center">AI Team at Decart</p><p class="text-white text-center">Etched: <a class="underline" href="">Julian Quevedo</a>, <a class="underline" href="">Quinn McIntyre</a>, <a class="underline" href="">Spruce Campbell</a>, <a class="underline" href="">Xinlei Chen</a>, <a class="underline" href="">Robert Wachen</a></p></div></div></div><script src="/_next/static/chunks/webpack-0954c5d36ad957a9.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/a34f9d1faa5f3315-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/css/8a84935111142e76.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"3:I[2846,[],\"\"]\n5:I[5878,[\"310\",\"static/chunks/0e5ce63c-a42bf838a066fc6f.js\",\"401\",\"static/chunks/401-82f646aab98d244d.js\",\"931\",\"static/chunks/app/page-b9eeb16764b91169.js\"],\"Image\"]\n6:I[6677,[\"310\",\"static/chunks/0e5ce63c-a42bf838a066fc6f.js\",\"401\",\"static/chunks/401-82f646aab98d244d.js\",\"931\",\"static/chunks/app/page-b9eeb16764b91169.js\"],\"Carousel\"]\n7:I[6677,[\"310\",\"static/chunks/0e5ce63c-a42bf838a066fc6f.js\",\"401\",\"static/chunks/401-82f646aab98d244d.js\",\"931\",\"static/chunks/app/page-b9eeb16764b91169.js\"],\"CarouselContent\"]\n8:I[6677,[\"310\",\"static/chunks/0e5ce63c-a42bf838a066fc6f.js\",\"401\",\"static/chunks/401-82f646aab98d244d.js\",\"931\",\"static/chunks/app/page-b9eeb16764b91169.js\"],\"CarouselItem\"]\n9:I[6677,[\"310\",\"static/chunks/0e5ce63c-a42bf838a066fc6f.js\",\"401\",\"static/chunks/401-82f646aab98d244d.js\",\"931\",\"static/chunks/app/page-b9eeb16764b91169.js\"],\"CarouselPrevious\"]\na:I[6677,[\"310\",\"static/chunks/0e5ce63c-a42bf838a066fc6f.js\",\"401\",\"static/chunks/401-82f646aab98d244d.js\",\"931\",\"static/chunks/app/page-b9eeb16764b91169.js\"],\"CarouselNext\"]\nb:I[4707,[],\"\"]\nc:I[6423,[],\"\"]\ne:I[1060,[],\"\"]\nf:[]\n"])</script><script>self.__next_f.push([1,"0:[\"$\",\"$L3\",null,{\"buildId\":\"cvF_tQoLxEIIndhM2MuY6\",\"assetPrefix\":\"\",\"urlParts\":[\"\",\"\"],\"initialTree\":[\"\",{\"children\":[\"__PAGE__\",{}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"__PAGE__\",{},[[\"$L4\",[\"$\",\"div\",null,{\"className\":\"\",\"children\":[[\"$\",\"main\",null,{\"className\":\"flex flex-col gap-8  p-8 pb-20 gap-16 sm:p-20 md:px-48 \",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-col items-center gap-8\",\"children\":[[\"$\",\"h3\",null,{\"children\":\"October 31, 2024\"}],[\"$\",\"h1\",null,{\"className\":\"text-5xl font-bold text-center\",\"children\":\"Oasis: A Universe in a Transformer\"}],[\"$\",\"h2\",null,{\"className\":\"mt-[-20px] text-center mb-8\",\"children\":[[\"$\",\"a\",null,{\"className\":\"underline\",\"href\":\"https://decart.ai\",\"children\":\"Decart\"}],\", \",[\"$\",\"a\",null,{\"className\":\"underline\",\"href\":\"https://etched.com\",\"children\":\"Etched\"}]]}],[\"$\",\"video\",null,{\"src\":\"/wide1.mp4\",\"autoPlay\":true,\"loop\":true,\"muted\":true,\"playsInline\":true,\"className\":\"w-full h-full object-cover\"}],[\"$\",\"div\",null,{\"className\":\"flex flex-col items-center justify-center gap-4 w-full md:w-1/2\",\"children\":[\"$\",\"a\",null,{\"className\":\"rounded-full border border-solid border-transparent transition-colors flex items-center justify-center bg-foreground text-background gap-2 hover:bg-[#383838] dark:hover:bg-[#ccc] text-sm sm:text-base h-14 sm:h-16 px-4 sm:px-5 w-56\",\"href\":\"\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":[[\"$\",\"svg\",null,{\"width\":\"15\",\"height\":\"15\",\"viewBox\":\"0 0 15 15\",\"fill\":\"none\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"className\":\"w-4 h-4\",\"children\":[\"$\",\"path\",null,{\"d\":\"M3.24182 2.32181C3.3919 2.23132 3.5784 2.22601 3.73338 2.30781L12.7334 7.05781C12.8974 7.14436 13 7.31457 13 7.5C13 7.68543 12.8974 7.85564 12.7334 7.94219L3.73338 12.6922C3.5784 12.774 3.3919 12.7687 3.24182 12.6782C3.09175 12.5877 3 12.4252 3 12.25V2.75C3 2.57476 3.09175 2.4123 3.24182 2.32181ZM4 3.57925V11.4207L11.4288 7.5L4 3.57925Z\",\"fill\":\"currentColor\",\"fillRule\":\"evenodd\",\"clipRule\":\"evenodd\"}]}],[\"$\",\"b\",null,{\"children\":\"Play demo\"}]]}]}],[\"$\",\"div\",null,{\"className\":\"flex gap-4 flex-col sm:flex-row w-full items-center justify-center mt-[-14px]\",\"children\":[[\"$\",\"a\",null,{\"className\":\"rounded-full border border-solid border-foreground transition-colors flex items-center justify-center gap-2 hover:bg-[#ccc] dark:hover:bg-[#383838] text-sm sm:text-base h-10 sm:h-12 px-4 sm:px-5\",\"href\":\"\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":[[\"$\",\"svg\",null,{\"width\":\"15\",\"height\":\"15\",\"viewBox\":\"0 0 15 15\",\"fill\":\"none\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"className\":\"w-4 h-4\",\"children\":[\"$\",\"path\",null,{\"d\":\"M3.64645 11.3536C3.45118 11.1583 3.45118 10.8417 3.64645 10.6465L10.2929 4L6 4C5.72386 4 5.5 3.77614 5.5 3.5C5.5 3.22386 5.72386 3 6 3L11.5 3C11.6326 3 11.7598 3.05268 11.8536 3.14645C11.9473 3.24022 12 3.36739 12 3.5L12 9.00001C12 9.27615 11.7761 9.50001 11.5 9.50001C11.2239 9.50001 11 9.27615 11 9.00001V4.70711L4.35355 11.3536C4.15829 11.5488 3.84171 11.5488 3.64645 11.3536Z\",\"fill\":\"currentColor\",\"fillRule\":\"evenodd\",\"clipRule\":\"evenodd\"}]}],\"Decart Blog\"]}],[\"$\",\"a\",null,{\"className\":\"rounded-full border border-solid border-foreground transition-colors flex items-center justify-center gap-2 hover:bg-[#ccc] dark:hover:bg-[#383838] text-sm sm:text-base h-10 sm:h-12 px-4 sm:px-5\",\"href\":\"\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":[[\"$\",\"$L5\",null,{\"className\":\"dark:invert\",\"src\":\"https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg\",\"alt\":\"GitHub\",\"width\":20,\"height\":20}],\"View code\"]}],[\"$\",\"a\",null,{\"className\":\"rounded-full border border-solid border-foreground transition-colors flex items-center justify-center gap-2 hover:bg-[#ccc] dark:hover:bg-[#383838] text-sm sm:text-base h-10 sm:h-12 px-4 sm:px-5\",\"href\":\"\",\"target\":\"https://etched.com/blog-posts/oasis\",\"rel\":\"noopener noreferrer\",\"children\":[[\"$\",\"svg\",null,{\"width\":\"15\",\"height\":\"15\",\"viewBox\":\"0 0 15 15\",\"fill\":\"none\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"className\":\"w-4 h-4\",\"children\":[\"$\",\"path\",null,{\"d\":\"M3.64645 11.3536C3.45118 11.1583 3.45118 10.8417 3.64645 10.6465L10.2929 4L6 4C5.72386 4 5.5 3.77614 5.5 3.5C5.5 3.22386 5.72386 3 6 3L11.5 3C11.6326 3 11.7598 3.05268 11.8536 3.14645C11.9473 3.24022 12 3.36739 12 3.5L12 9.00001C12 9.27615 11.7761 9.50001 11.5 9.50001C11.2239 9.50001 11 9.27615 11 9.00001V4.70711L4.35355 11.3536C4.15829 11.5488 3.84171 11.5488 3.64645 11.3536Z\",\"fill\":\"currentColor\",\"fillRule\":\"evenodd\",\"clipRule\":\"evenodd\"}]}],\"Etched Blog\"]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"flex flex-col gap-4 px-0 lg:px-32\",\"children\":[[\"$\",\"p\",null,{\"className\":\"text-justify leading-relaxed\",\"children\":\"We're excited to announce Oasis, the first playable, realtime, open-world AI model. It's a video game, but entirely generated by AI. Oasis is the first step in our research towards more complex interactive worlds. \"}],[\"$\",\"p\",null,{\"className\":\"text-justify leading-relaxed\",\"children\":\"Oasis takes in user keyboard input and generates real-time gameplay, including physics, game rules, and graphics. You can move around, jump, pick up items, break blocks, and more. There is no game engine; just a foundation model.\"}],[\"$\",\"p\",null,{\"className\":\"text-justify leading-relaxed\",\"children\":\"We believe fast transformer inference is the missing link to making generative video a reality. Today, using Decart's inference engine, we show that real-time video is possible. When Etched's transformer ASIC, Sohu, is released, we can run models like Oasis in 4K. Today, we're releasing Oasis's code, the weights of a 500M parameter model you can run locally, and a live playable demo of a larger checkpoint.\"}],[\"$\",\"h2\",null,{\"className\":\"text-2xl font-bold text-center mt-8\",\"children\":\"Gameplay Results\"}],[\"$\",\"div\",null,{\"className\":\"relative w-screen left-1/2 right-1/2 -mx-[50vw] my-4\",\"children\":[\"$\",\"$L6\",null,{\"className\":\"w-full\",\"opts\":{\"loop\":true},\"children\":[[\"$\",\"$L7\",null,{\"children\":[[\"$\",\"$L8\",\"0\",{\"className\":\"basis-1/2 md:basis-1/3\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex flex-col items-center justify-center\",\"children\":[[\"$\",\"$L5\",null,{\"src\":\"/placing_4_fences.webp\",\"alt\":\"Placing non-cube blocks\",\"width\":700,\"height\":300,\"className\":\"object-cover\"}],[\"$\",\"p\",null,{\"className\":\"text-sm text-center p-2\",\"children\":\"Placing non-cube blocks\"}]]}]}],[\"$\",\"$L8\",\"1\",{\"className\":\"basis-1/2 md:basis-1/3\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex flex-col items-center justify-center\",\"children\":[[\"$\",\"$L5\",null,{\"src\":\"/torch_becomes_dark.webp\",\"alt\":\"Model understands lighting physics\",\"width\":700,\"height\":300,\"className\":\"object-cover\"}],[\"$\",\"p\",null,{\"className\":\"text-sm text-center p-2\",\"children\":\"Model understands lighting physics\"}]]}]}],[\"$\",\"$L8\",\"2\",{\"className\":\"basis-1/2 md:basis-1/3\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex flex-col items-center justify-center\",\"children\":[[\"$\",\"$L5\",null,{\"src\":\"/inventory_and_changing_hands.webp\",\"alt\":\"Real-time block manipulation\",\"width\":700,\"height\":300,\"className\":\"object-cover\"}],[\"$\",\"p\",null,{\"className\":\"text-sm text-center p-2\",\"children\":\"Real-time block manipulation\"}]]}]}],[\"$\",\"$L8\",\"3\",{\"className\":\"basis-1/2 md:basis-1/3\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex flex-col items-center justify-center\",\"children\":[[\"$\",\"$L5\",null,{\"src\":\"/2.webp\",\"alt\":\"Interactive crafting system\",\"width\":700,\"height\":300,\"className\":\"object-cover\"}],[\"$\",\"p\",null,{\"className\":\"text-sm text-center p-2\",\"children\":\"Interactive crafting system\"}]]}]}],[\"$\",\"$L8\",\"4\",{\"className\":\"basis-1/2 md:basis-1/3\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex flex-col items-center justify-center\",\"children\":[[\"$\",\"$L5\",null,{\"src\":\"/3.webp\",\"alt\":\"AI-driven NPCs and wildlife\",\"width\":700,\"height\":300,\"className\":\"object-cover\"}],[\"$\",\"p\",null,{\"className\":\"text-sm text-center p-2\",\"children\":\"AI-driven NPCs and wildlife\"}]]}]}]]}],[\"$\",\"div\",null,{\"className\":\"flex justify-center gap-2 mt-4\",\"children\":[[\"$\",\"$L9\",null,{\"className\":\"static translate-y-0\"}],[\"$\",\"$La\",null,{\"className\":\"static translate-y-0\"}]]}]]}]}],[\"$\",\"p\",null,{\"className\":\"text-justify leading-relaxed\",\"children\":\"Our model even understands complex game mechanics, such as crafting, and NPCs ADD MORE EXAMPLES.\"}],[\"$\",\"div\",null,{\"className\":\"relative w-screen left-1/2 right-1/2 -mx-[50vw] my-4\",\"children\":[\"$\",\"$L6\",null,{\"className\":\"w-full\",\"opts\":{\"loop\":true},\"children\":[[\"$\",\"$L7\",null,{\"children\":[[\"$\",\"$L8\",\"0\",{\"className\":\"basis-1/2 md:basis-1/3\",\"children\":[\"$\",\"div\",null,{\"className\":\"rounded-xl border bg-card text-card-foreground shadow\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex items-center justify-center p-0\",\"children\":[\"$\",\"$L5\",null,{\"src\":\"/1.webp\",\"alt\":\"Carousel image 1\",\"width\":700,\"height\":300,\"className\":\"object-cover\"}]}]}]}],[\"$\",\"$L8\",\"1\",{\"className\":\"basis-1/2 md:basis-1/3\",\"children\":[\"$\",\"div\",null,{\"className\":\"rounded-xl border bg-card text-card-foreground shadow\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex items-center justify-center p-0\",\"children\":[\"$\",\"$L5\",null,{\"src\":\"/2.webp\",\"alt\":\"Carousel image 2\",\"width\":700,\"height\":300,\"className\":\"object-cover\"}]}]}]}],[\"$\",\"$L8\",\"2\",{\"className\":\"basis-1/2 md:basis-1/3\",\"children\":[\"$\",\"div\",null,{\"className\":\"rounded-xl border bg-card text-card-foreground shadow\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex items-center justify-center p-0\",\"children\":[\"$\",\"$L5\",null,{\"src\":\"/3.webp\",\"alt\":\"Carousel image 3\",\"width\":700,\"height\":300,\"className\":\"object-cover\"}]}]}]}],[\"$\",\"$L8\",\"3\",{\"className\":\"basis-1/2 md:basis-1/3\",\"children\":[\"$\",\"div\",null,{\"className\":\"rounded-xl border bg-card text-card-foreground shadow\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex items-center justify-center p-0\",\"children\":[\"$\",\"$L5\",null,{\"src\":\"/4.webp\",\"alt\":\"Carousel image 4\",\"width\":700,\"height\":300,\"className\":\"object-cover\"}]}]}]}],[\"$\",\"$L8\",\"4\",{\"className\":\"basis-1/2 md:basis-1/3\",\"children\":[\"$\",\"div\",null,{\"className\":\"rounded-xl border bg-card text-card-foreground shadow\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex items-center justify-center p-0\",\"children\":[\"$\",\"$L5\",null,{\"src\":\"/5.webp\",\"alt\":\"Carousel image 5\",\"width\":700,\"height\":300,\"className\":\"object-cover\"}]}]}]}]]}],[\"$\",\"div\",null,{\"className\":\"flex justify-center gap-2 mt-4\",\"children\":[[\"$\",\"$L9\",null,{\"className\":\"static translate-y-0\"}],[\"$\",\"$La\",null,{\"className\":\"static translate-y-0\"}]]}]]}]}],[\"$\",\"h2\",null,{\"className\":\"text-2xl font-bold text-center mt-8\",\"children\":\"Architecture\"}],[\"$\",\"p\",null,{\"className\":\"text-justify leading-relaxed\",\"children\":[\"The model is composed of two parts: a spatial autoencoder, and a latent diffusion backbone. Both are Transformer-based: the autoencoder is based on ViT\",[\"$\",\"sup\",null,{\"className\":\"text-gray-500\",\"children\":\"[1]\"}],\", and the backbone is based on DiT\",[\"$\",\"sup\",null,{\"className\":\"text-gray-500\",\"children\":\"[2]\"}],\". We chose Transformers to ensure stable, predictable scaling, and fast inference on Etched's Transformer ASIC, Sohu.\"]}],[\"$\",\"$L5\",null,{\"src\":\"/arch_new.png\",\"alt\":\"Architecture\",\"width\":1000,\"height\":500}],[\"$\",\"p\",null,{\"className\":\"text-justify leading-relaxed\",\"children\":[\"In contrast to bidirectional models such as Sora\",[\"$\",\"sup\",null,{\"className\":\"text-gray-500\",\"children\":\"[3]\"}],\", Oasis generates frames autoregressively, with the ability to condition each frame on game input. This enables users to interact with the world in real-time. The model was trained using Diffusion Forcing\",[\"$\",\"sup\",null,{\"className\":\"text-gray-500\",\"children\":\"[4]\"}],\", which denoises with independent per-token noise levels, and allows for novel decoding schemes such as ours.\"]}],[\"$\",\"p\",null,{\"className\":\"text-justify leading-relaxed\",\"children\":\"One issue we focused on is temporal stability--making sure the model outputs make sense over long time horizons. In autoregressive models, errors compound, and small imperfections can quickly snowball into glitched frames. Solving this required innovations in long-context generation.\"}],[\"$\",\"p\",null,{\"className\":\"text-justify leading-relaxed\",\"children\":\"We solved this by deploying dynamic noising, which adjusts inference-time noise on a schedule, injecting noise in the first diffusion forward passes to reduce error accumulation, and gradually removing noise in the later passes so the model can find and persist high-frequency details in previous frames for improved consistency. Since our model saw noise during training, it learned to successfully deal with noisy samples at inference.\"}],[\"$\",\"div\",null,{\"className\":\"flex justify-center items-center\",\"children\":[\"$\",\"$L5\",null,{\"src\":\"/dyno.png\",\"alt\":\"Dynamic Noising\",\"width\":700,\"height\":500}]}],[\"$\",\"p\",null,{\"className\":\"text-justify leading-relaxed\",\"children\":[\"To learn more about the engineering underlying this model, and some of the specific optimizations in training and inference, check out the \",[\"$\",\"a\",null,{\"className\":\"underline\",\"href\":\"https://decart.ai/blog/training-the-world-model\",\"children\":\"Decart blog post\"}],\".\"]}],[\"$\",\"div\",null,{\"className\":\"relative w-screen left-1/2 right-1/2 -mx-[50vw] my-4\",\"children\":[\"$\",\"$L6\",null,{\"className\":\"w-full\",\"opts\":{\"loop\":true},\"children\":[[\"$\",\"$L7\",null,{\"children\":[[\"$\",\"$L8\",\"0\",{\"className\":\"basis-1/2 md:basis-1/3\",\"children\":[\"$\",\"div\",null,{\"className\":\"rounded-xl border bg-card text-card-foreground shadow\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex items-center justify-center p-0\",\"children\":[\"$\",\"$L5\",null,{\"src\":\"/5.webp\",\"alt\":\"Carousel image 1\",\"width\":700,\"height\":300,\"className\":\"object-cover\"}]}]}]}],[\"$\",\"$L8\",\"1\",{\"className\":\"basis-1/2 md:basis-1/3\",\"children\":[\"$\",\"div\",null,{\"className\":\"rounded-xl border bg-card text-card-foreground shadow\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex items-center justify-center p-0\",\"children\":[\"$\",\"$L5\",null,{\"src\":\"/4.webp\",\"alt\":\"Carousel image 2\",\"width\":700,\"height\":300,\"className\":\"object-cover\"}]}]}]}],[\"$\",\"$L8\",\"2\",{\"className\":\"basis-1/2 md:basis-1/3\",\"children\":[\"$\",\"div\",null,{\"className\":\"rounded-xl border bg-card text-card-foreground shadow\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex items-center justify-center p-0\",\"children\":[\"$\",\"$L5\",null,{\"src\":\"/1.webp\",\"alt\":\"Carousel image 3\",\"width\":700,\"height\":300,\"className\":\"object-cover\"}]}]}]}],[\"$\",\"$L8\",\"3\",{\"className\":\"basis-1/2 md:basis-1/3\",\"children\":[\"$\",\"div\",null,{\"className\":\"rounded-xl border bg-card text-card-foreground shadow\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex items-center justify-center p-0\",\"children\":[\"$\",\"$L5\",null,{\"src\":\"/2.webp\",\"alt\":\"Carousel image 4\",\"width\":700,\"height\":300,\"className\":\"object-cover\"}]}]}]}],[\"$\",\"$L8\",\"4\",{\"className\":\"basis-1/2 md:basis-1/3\",\"children\":[\"$\",\"div\",null,{\"className\":\"rounded-xl border bg-card text-card-foreground shadow\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex items-center justify-center p-0\",\"children\":[\"$\",\"$L5\",null,{\"src\":\"/3.webp\",\"alt\":\"Carousel image 5\",\"width\":700,\"height\":300,\"className\":\"object-cover\"}]}]}]}]]}],[\"$\",\"div\",null,{\"className\":\"flex justify-center gap-2 mt-4\",\"children\":[[\"$\",\"$L9\",null,{\"className\":\"static translate-y-0\"}],[\"$\",\"$La\",null,{\"className\":\"static translate-y-0\"}]]}]]}]}],[\"$\",\"h2\",null,{\"className\":\"text-2xl font-bold text-center mt-8\",\"children\":\"Performance\"}],[\"$\",\"p\",null,{\"className\":\"text-justify leading-relaxed\",\"children\":[\"Oasis generates real-time output in 24 frames per second. Current state-of-the-art text-to-video models with a similar DiT architecture (e.g. Sora\",[\"$\",\"sup\",null,{\"className\":\"text-gray-500\",\"children\":\"[3]\"}],\", Mochi-1\",[\"$\",\"sup\",null,{\"className\":\"text-gray-500\",\"children\":\"[6]\"}],\" and Runway\",[\"$\",\"sup\",null,{\"className\":\"text-gray-500\",\"children\":\"[7]\"}],\") can take 10-20 seconds to create just one second of video, even on multiple GPUs. In order to match the experience of playing a game, however, our model must generate a new frame every 0.04 seconds, which is over 100x faster.\"]}],[\"$\",\"section\",null,{\"className\":\"flex justify-center items-center my-4\",\"children\":[\"$\",\"$L5\",null,{\"src\":\"/speed.png\",\"alt\":\"Performance\",\"width\":500,\"height\":500}]}],[\"$\",\"p\",null,{\"className\":\"text-justify leading-relaxed\",\"children\":[\"With Decart's inference stack, the model runs at playable framerates, unlocking real-time interactivity for the first time. Read more about it on \",[\"$\",\"a\",null,{\"className\":\"underline\",\"href\":\"https://decart.ai/blog/training-the-world-model\",\"children\":\"Decart's blog\"}],\".\"]}],[\"$\",\"p\",null,{\"className\":\"text-justify leading-relaxed\",\"children\":\"However, to make the model an additional order of magnitude faster, and make it cost-efficient to run at scale, new hardware is needed. Oasis is optimized for Sohu, the Transformer ASIC built by Etched. On NVIDIA H100s today, the model can run at 720p at 20fps. Sohu can run the same model at up to 4K, which is 4x more tokens per second. \"}],[\"$\",\"p\",null,{\"className\":\"text-justify leading-relaxed\",\"children\":[\"In addition, Oasis' end-to-end Transformer architecture makes it extremely efficient on Sohu - at the same price and power consumption as an H100 GPU, Oasis on Sohu can serve 10x more users. We believe the price of serving models like Oasis is the hidden bottleneck to releasing generative video in production. See more performance figures and read more about Oasis and Sohu on \",[\"$\",\"a\",null,{\"className\":\"underline\",\"href\":\"https://etched.ai/blog\",\"children\":\"Etched's blog\"}],\".\"]}],[\"$\",\"section\",null,{\"className\":\"flex justify-center items-center my-4\",\"children\":[\"$\",\"$L5\",null,{\"src\":\"/sohu.png\",\"alt\":\"Performance\",\"width\":500,\"height\":500}]}],[\"$\",\"h2\",null,{\"className\":\"text-2xl font-bold text-center mt-8\",\"children\":\"Future Explorations\"}],[\"$\",\"p\",null,{\"className\":\"text-justify leading-relaxed\",\"children\":\" With the many exciting results, there come areas for future development in the model. There are difficulties with the sometimes fuzzy video in the distance, the temporal consistency of uncertain objects, domain generalization, and difficulties over long contexts.\"}],[\"$\",\"div\",null,{\"className\":\"relative w-screen left-1/2 right-1/2 -mx-[50vw] my-4\",\"children\":[\"$\",\"$L6\",null,{\"className\":\"w-full\",\"opts\":{\"loop\":true},\"children\":[[\"$\",\"$L7\",null,{\"children\":[[\"$\",\"$L8\",\"0\",{\"className\":\"basis-1/2 md:basis-1/3\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex flex-col items-center justify-center\",\"children\":[[\"$\",\"$L5\",null,{\"src\":\"/placing_4_fences.webp\",\"alt\":\"Placing non-cube blocks\",\"width\":700,\"height\":300,\"className\":\"object-cover\"}],[\"$\",\"p\",null,{\"className\":\"text-sm text-center p-2\",\"children\":\"Placing non-cube blocks\"}]]}]}],[\"$\",\"$L8\",\"1\",{\"className\":\"basis-1/2 md:basis-1/3\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex flex-col items-center justify-center\",\"children\":[[\"$\",\"$L5\",null,{\"src\":\"/torch_becomes_dark.webp\",\"alt\":\"Model understands lighting physics\",\"width\":700,\"height\":300,\"className\":\"object-cover\"}],[\"$\",\"p\",null,{\"className\":\"text-sm text-center p-2\",\"children\":\"Model understands lighting physics\"}]]}]}],[\"$\",\"$L8\",\"2\",{\"className\":\"basis-1/2 md:basis-1/3\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex flex-col items-center justify-center\",\"children\":[[\"$\",\"$L5\",null,{\"src\":\"/inventory_and_changing_hands.webp\",\"alt\":\"Real-time block manipulation\",\"width\":700,\"height\":300,\"className\":\"object-cover\"}],[\"$\",\"p\",null,{\"className\":\"text-sm text-center p-2\",\"children\":\"Real-time block manipulation\"}]]}]}],[\"$\",\"$L8\",\"3\",{\"className\":\"basis-1/2 md:basis-1/3\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex flex-col items-center justify-center\",\"children\":[[\"$\",\"$L5\",null,{\"src\":\"/2.webp\",\"alt\":\"Interactive crafting system\",\"width\":700,\"height\":300,\"className\":\"object-cover\"}],[\"$\",\"p\",null,{\"className\":\"text-sm text-center p-2\",\"children\":\"Interactive crafting system\"}]]}]}],[\"$\",\"$L8\",\"4\",{\"className\":\"basis-1/2 md:basis-1/3\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex flex-col items-center justify-center\",\"children\":[[\"$\",\"$L5\",null,{\"src\":\"/3.webp\",\"alt\":\"AI-driven NPCs and wildlife\",\"width\":700,\"height\":300,\"className\":\"object-cover\"}],[\"$\",\"p\",null,{\"className\":\"text-sm text-center p-2\",\"children\":\"AI-driven NPCs and wildlife\"}]]}]}]]}],[\"$\",\"div\",null,{\"className\":\"flex justify-center gap-2 mt-4\",\"children\":[[\"$\",\"$L9\",null,{\"className\":\"static translate-y-0\"}],[\"$\",\"$La\",null,{\"className\":\"static translate-y-0\"}]]}]]}]}],[\"$\",\"h2\",null,{\"className\":\"text-2xl font-bold text-center mt-8\",\"children\":\"What's Next\"}],[\"$\",\"p\",null,{\"className\":\"text-justify leading-relaxed\",\"children\":\"Oasis is an impressive technical demo, but we believe this research will enable an exciting new generation of foundation models and consumer products. For example:\"}],[\"$\",\"ul\",null,{\"className\":\"list-disc list-inside\",\"children\":[[\"$\",\"li\",null,{\"children\":\"Creating and editing game content on-the-fly, even while playing, through text and image prompting\"}],[\"$\",\"li\",null,{\"children\":\"Generating content individually tailored for each user on social media\"}],[\"$\",\"li\",null,{\"children\":\"Real-time medical care, able to respond on the fly to patients in a video call\"}],[\"$\",\"li\",null,{\"children\":\"AI teachers, who can generate videos to respond to students in a classroom\"}]]}],[\"$\",\"p\",null,{\"className\":\"text-justify leading-relaxed\",\"children\":\"Oasis is the first in a series of world generation models - we're scaling our dataset and architecture by 10x each, and we're excited to report our findings soon.\"}],[\"$\",\"p\",null,{\"className\":\"text-justify leading-relaxed\",\"children\":[\"Etched and Decart are excited to build these models together. The integration of Oasis with our hardware and software, from architecture to production, will ensure this model family remains one of the fastest and best as we advance the frontier of world generation. If you're interested in collaborating, reach out to \",[\"$\",\"a\",null,{\"href\":\"mailto:tal@decart.ai\",\"children\":\"tal@decart.ai\"}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"flex flex-col gap-4 p-8 sm:p-20 md:px-48 bg-secondary\",\"children\":[[\"$\",\"p\",null,{\"className\":\"text-justify text-sm text-secondary-foreground\",\"children\":[\"[1]: \",[\"$\",\"a\",null,{\"className\":\"underline\",\"href\":\"https://arxiv.org/abs/2010.11929\",\"children\":\"Dosovitskiy et al., An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\"}]]}],[\"$\",\"p\",null,{\"className\":\"text-justify text-sm text-secondary-foreground\",\"children\":[\"[2]: \",[\"$\",\"a\",null,{\"className\":\"underline\",\"href\":\"https://arxiv.org/abs/2212.09748\",\"children\":\"Peebles et al., Scalable Diffusion Models with Transformers\"}]]}],[\"$\",\"p\",null,{\"className\":\"text-justify text-sm text-secondary-foreground\",\"children\":[\"[3]: \",[\"$\",\"a\",null,{\"className\":\"underline\",\"href\":\"https://openai.com/index/video-generation-models-as-world-simulators/\",\"children\":\"OpenAI, Video generation models as world simulators\"}]]}],[\"$\",\"p\",null,{\"className\":\"text-justify text-sm text-secondary-foreground\",\"children\":[\"[4]: \",[\"$\",\"a\",null,{\"className\":\"underline\",\"href\":\"https://arxiv.org/abs/2407.01392\",\"children\":\"Chen et al., Diffusion Forcing: Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion\"}]]}],[\"$\",\"p\",null,{\"className\":\"text-justify text-sm text-secondary-foreground\",\"children\":[\"[5]: \",[\"$\",\"a\",null,{\"className\":\"underline\",\"href\":\"https://arxiv.org/abs/2211.05102\",\"children\":\"Pope et al., Efficiently Scaling Transformer Inference\"}]]}],[\"$\",\"p\",null,{\"className\":\"text-justify text-sm text-secondary-foreground\",\"children\":[\"[6]: \",[\"$\",\"a\",null,{\"className\":\"underline\",\"href\":\"https://www.genmo.ai/blog\",\"children\":\"Genmo, Mochi 1: A new SOTA in open-source video generation models\"}]]}],[\"$\",\"p\",null,{\"className\":\"text-justify text-sm text-secondary-foreground\",\"children\":[\"[7]: \",[\"$\",\"a\",null,{\"className\":\"underline\",\"href\":\"https://runwayml.com/research/introducing-gen-3-alpha\",\"children\":\"Runway, Introducing Gen-3 Alpha: A New Frontier for Video Generation\"}]]}],[\"$\",\"p\",null,{\"className\":\"text-justify text-sm text-secondary-foreground\",\"children\":\"* Estimated throughput figures - Sora reported, Mochi-1 from FAL.AI endpoint adjusted for parameter count, Runway from Gen-3 reported throughput\"}]]}],[\"$\",\"div\",null,{\"className\":\"flex flex-col gap-4 px-0 md:px-16 bg-black\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex flex-col gap-4 p-8 pb-20 gap-16 sm:p-20 md:px-48\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-2xl font-bold text-center text-white\",\"children\":\"Contributors\"}],[\"$\",\"p\",null,{\"className\":\"text-white text-center\",\"children\":\"AI Team at Decart\"}],[\"$\",\"p\",null,{\"className\":\"text-white text-center\",\"children\":[\"Etched: \",[\"$\",\"a\",null,{\"className\":\"underline\",\"href\":\"\",\"children\":\"Julian Quevedo\"}],\", \",[\"$\",\"a\",null,{\"className\":\"underline\",\"href\":\"\",\"children\":\"Quinn McIntyre\"}],\", \",[\"$\",\"a\",null,{\"className\":\"underline\",\"href\":\"\",\"children\":\"Spruce Campbell\"}],\", \",[\"$\",\"a\",null,{\"className\":\"underline\",\"href\":\"\",\"children\":\"Xinlei Chen\"}],\", \",[\"$\",\"a\",null,{\"className\":\"underline\",\"href\":\"\",\"children\":\"Robert Wachen\"}]]}]]}]}]]}],null],null],null]},[[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/8a84935111142e76.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"__className_d65c78 antialiased\",\"children\":[\"$\",\"$Lb\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$Lc\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[]}]}]}]],null],null],\"couldBeIntercepted\":false,\"initialHead\":[null,\"$Ld\"],\"globalErrorComponent\":\"$e\",\"missingSlots\":\"$Wf\"}]\n"])</script><script>self.__next_f.push([1,"d:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"Oasis\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"Generating Worlds in Realtime\"}],[\"$\",\"link\",\"4\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"16x16\"}],[\"$\",\"meta\",\"5\",{\"name\":\"next-size-adjust\"}]]\n4:null\n"])</script></body></html>