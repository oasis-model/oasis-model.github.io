<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/a34f9d1faa5f3315-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" as="image" href="/image4.gif" fetchPriority="high"/><link rel="stylesheet" href="/_next/static/css/17e25edc667feb7e.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-0954c5d36ad957a9.js"/><script src="/_next/static/chunks/fd9d1056-3f777ab9f9509526.js" async=""></script><script src="/_next/static/chunks/117-b4af6d3f17e58014.js" async=""></script><script src="/_next/static/chunks/main-app-db45bcc7c468b235.js" async=""></script><script src="/_next/static/chunks/0e5ce63c-a42bf838a066fc6f.js" async=""></script><script src="/_next/static/chunks/401-82f646aab98d244d.js" async=""></script><script src="/_next/static/chunks/app/page-816450e1d70f9500.js" async=""></script><title>Oasis</title><meta name="description" content="Generating Worlds in Realtime"/><link rel="icon" href="/favicon.ico" type="image/x-icon" sizes="16x16"/><meta name="next-size-adjust"/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="__className_d65c78 antialiased"><div class=""><main class="flex flex-col gap-8  p-8 pb-20 gap-16 sm:p-20 md:px-48 "><div class="flex flex-col items-center gap-8"><h3>October 29, 2024</h3><h1 class="text-5xl font-bold text-center">Oasis: Generating Worlds in Realtime</h1><h2 class="mt-[-20px] text-center mb-8">Etched Team, Decart Team</h2><img alt="Next.js logo" fetchPriority="high" width="600" height="400" decoding="async" data-nimg="1" class="dark:invert" style="color:transparent" src="/image4.gif"/><div class="flex gap-4 items-center flex-col sm:flex-row"><a class="rounded-full border border-solid border-transparent transition-colors flex items-center justify-center bg-foreground text-background gap-2 hover:bg-[#383838] dark:hover:bg-[#ccc] text-sm sm:text-base h-10 sm:h-12 px-4 sm:px-5" href="" target="_blank" rel="noopener noreferrer"><img alt="GitHub" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="dark:invert" style="color:transparent;filter:invert(100%)" src="https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg"/>View code</a><a class="rounded-full border border-solid border-transparent transition-colors flex items-center justify-center bg-foreground text-background gap-2 hover:bg-[#383838] dark:hover:bg-[#ccc] text-sm sm:text-base h-10 sm:h-12 px-4 sm:px-5" href="" target="_blank" rel="noopener noreferrer">Play demo</a><a class="rounded-full border border-solid border-black/[.08] dark:border-white/[.145] transition-colors flex items-center justify-center hover:bg-[#f2f2f2] dark:hover:bg-[#1a1a1a] hover:border-transparent text-sm sm:text-base text-gray-500 h-10 sm:h-12 px-4 sm:px-5 sm:min-w-44" href="https://nextjs.org/docs?utm_source=create-next-app&amp;utm_medium=appdir-template-tw&amp;utm_campaign=create-next-app" target="_blank" rel="noopener noreferrer">Paper (Coming Soon)</a></div></div><div class="flex flex-col gap-4 px-0 lg:px-32"><p class="text-justify leading-relaxed">We have trained a new model, Oasis, that can generate 3D interactive worlds in realtime. In this technical report, we will discuss the model behavior and results, detail the architecture and training process, and release the code open-source. </p><p class="text-justify leading-relaxed">Oasis is the first playable fully AI-based game engine. Streaming interactive AI-generated video to users is a technical challenge on par with developing the model itself, and in addition to our architecture, this report will detail the technical innovations that will be required to serve video models at scale, and our vision for the future of interactive content. </p><div class="flex justify-center items-center mt-8"><div class="relative w-full" role="region" aria-roledescription="carousel"><div class="overflow-hidden"><div class="flex -ml-4"><div role="group" aria-roledescription="slide" class="min-w-0 shrink-0 grow-0 pl-4 basis-1/3"><div class="rounded-xl border bg-card text-card-foreground shadow"><div class="flex items-center justify-center p-0"><img alt="Carousel image 1" loading="lazy" width="700" height="300" decoding="async" data-nimg="1" class="object-cover" style="color:transparent" src="/image2.gif"/></div></div></div><div role="group" aria-roledescription="slide" class="min-w-0 shrink-0 grow-0 pl-4 basis-1/3"><div class="rounded-xl border bg-card text-card-foreground shadow"><div class="flex items-center justify-center p-0"><img alt="Carousel image 2" loading="lazy" width="700" height="300" decoding="async" data-nimg="1" class="object-cover" style="color:transparent" src="/image3.gif"/></div></div></div><div role="group" aria-roledescription="slide" class="min-w-0 shrink-0 grow-0 pl-4 basis-1/3"><div class="rounded-xl border bg-card text-card-foreground shadow"><div class="flex items-center justify-center p-0"><img alt="Carousel image 3" loading="lazy" width="700" height="300" decoding="async" data-nimg="1" class="object-cover" style="color:transparent" src="/image4.gif"/></div></div></div></div></div><button class="inline-flex items-center justify-center gap-2 whitespace-nowrap text-sm font-medium transition-colors focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg]:size-4 [&amp;_svg]:shrink-0 border border-input bg-background shadow-sm hover:bg-accent hover:text-accent-foreground absolute h-8 w-8 rounded-full -left-12 top-1/2 -translate-y-1/2" disabled=""><svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg" class="h-4 w-4"><path d="M6.85355 3.14645C7.04882 3.34171 7.04882 3.65829 6.85355 3.85355L3.70711 7H12.5C12.7761 7 13 7.22386 13 7.5C13 7.77614 12.7761 8 12.5 8H3.70711L6.85355 11.1464C7.04882 11.3417 7.04882 11.6583 6.85355 11.8536C6.65829 12.0488 6.34171 12.0488 6.14645 11.8536L2.14645 7.85355C1.95118 7.65829 1.95118 7.34171 2.14645 7.14645L6.14645 3.14645C6.34171 2.95118 6.65829 2.95118 6.85355 3.14645Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg><span class="sr-only">Previous slide</span></button><button class="inline-flex items-center justify-center gap-2 whitespace-nowrap text-sm font-medium transition-colors focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg]:size-4 [&amp;_svg]:shrink-0 border border-input bg-background shadow-sm hover:bg-accent hover:text-accent-foreground absolute h-8 w-8 rounded-full -right-12 top-1/2 -translate-y-1/2" disabled=""><svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg" class="h-4 w-4"><path d="M8.14645 3.14645C8.34171 2.95118 8.65829 2.95118 8.85355 3.14645L12.8536 7.14645C13.0488 7.34171 13.0488 7.65829 12.8536 7.85355L8.85355 11.8536C8.65829 12.0488 8.34171 12.0488 8.14645 11.8536C7.95118 11.6583 7.95118 11.3417 8.14645 11.1464L11.2929 8H2.5C2.22386 8 2 7.77614 2 7.5C2 7.22386 2.22386 7 2.5 7H11.2929L8.14645 3.85355C7.95118 3.65829 7.95118 3.34171 8.14645 3.14645Z" fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path></svg><span class="sr-only">Next slide</span></button></div></div><h2 class="text-2xl font-bold text-center mt-8">Architecture</h2><img alt="Architecture" loading="lazy" width="1000" height="500" decoding="async" data-nimg="1" style="color:transparent" src="/arch.png"/></div><div class="flex flex-col gap-4 px-0 md:px-16"><p>The model is composed of two parts - an autoencoding patch tokenizer and a latent diffusion backbone. Notably, both are based on Transformer architectures - the autoencoder is based on <a class="underline" href="https://arxiv.org/abs/2010.11929">ViT</a> (Vision Transformer), and the backbone is based on <a class="underline" href="https://arxiv.org/abs/2212.09748">DiT</a> (Diffusion Transformer). Conventionally, image models (such as <a class="underline" href="https://arxiv.org/abs/2112.10752">Stable Diffusion</a>) use VAEs and UNets, but we explicitly chose Transformers to ensure stable, predictable scaling laws, and fast inference on Sohu. </p></div><h3 class="text-xl font-bold text-center mt-8">Tokenizer</h3><div class="flex flex-col gap-4 px-0 md:px-16"><p>We experimented with novel approaches to tokenization to enable higher resolution and higher quality. We anticipated that more compute spent on patching would result in higher-quality output. To do this, we modified a Vision Transformer, reducing the dimensionality of the central layer and training it as a VAE. We enforce a dimensionality bottleneck and Gaussian distribution on the middle layer, using it to tokenize 10-by-10 pixel patches into latent tokens for the diffusion model. </p><p>We train our tokenizer on 1 frame of every second of our dataset, such that the model does not overfit to features in particular videos. We stop training once the validation quality gets worse and the model has overfit.</p><img alt="Tokenizer" loading="lazy" width="1000" height="500" decoding="async" data-nimg="1" style="color:transparent" src="/tokenizer.png"/><p>We observe strong locality in the features in our tokenizers, namely that individual tokens often correspond directly to local regions in space. This allows us to reverse engineer feature attribution within the model. Once these features are understood, we can edit the model’s output simply by editing the latent token space. </p><p>An example here is selecting a blank item bar, and inserting a full toolbar in the menu via the latent representations:</p><img alt="Toolbar" loading="lazy" width="1000" height="500" decoding="async" data-nimg="1" style="color:transparent" src="/toolbar.png"/></div><h3 class="text-xl font-bold text-center mt-8">Diffusion Backbone</h3><div class="flex flex-col gap-4 px-0 md:px-16"><p>In contrast to bidirectional models such as Sora, Oasis can generate an unlimited length of video autoregressively, with the option for each frame to be conditioned on game input. This enables users to interact with the world in realtime. This behavior is based on the <a class="underline" href="https://arxiv.org/abs/2408.00927">Diffusion Forcing</a> paper, which proposed models that simultaneously predict future tokens while denoising past ones:</p><img alt="Forcing" loading="lazy" width="1000" height="500" decoding="async" data-nimg="1" style="color:transparent" src="/forcing.png"/><p>We replicate the Diffusion Forcing DiT backbone with some minor adjustments for speed and stability. In particular, we train 500M and 1B parameter checkpoints, for 200,000 steps, over one epoch of gameplay data.        </p><p>During development, we attempted to use an off-the-shelf LLaMA architecture to predict video frames from a discrete codebook, similar to the approach taken in LLaMA-Gen. While we found the model trained well and we were able to take advantage of the optimisation of many open-source libraries around LLaMA models, we were unable to get long-duration temporal consistency in the model’s outputs. For this reason, we chose to use a continuous architecture. We believe there are natural architectures for continuous data domains such as video and images, and that “vanilla” GPT models will not trivially generalize to this task.</p></div><h2 class="text-2xl font-bold text-center mt-8">Results</h2><h3 class="text-xl font-bold text-center mt-8">500M</h3><div class="flex flex-col gap-4 px-0 md:px-16"><p>At the 500M parameter scale, there are recognizable features, but there is a high-degree of jitter, and the model struggles to remain coherent between frames:</p><div class="flex justify-center items-center"><img alt="500M" loading="lazy" width="1000" height="500" decoding="async" data-nimg="1" style="color:transparent" src="/500m.gif"/><img alt="500M" loading="lazy" width="1000" height="500" decoding="async" data-nimg="1" style="color:transparent" src="/500m-2.gif"/><img alt="500M" loading="lazy" width="1000" height="500" decoding="async" data-nimg="1" style="color:transparent" src="/500m-3.gif"/></div></div><h3 class="text-xl font-bold text-center mt-8">1B</h3><div class="flex flex-col gap-4 px-0 md:px-16"><p>At the 1B scale, the model begins to capture more complex behaviors and objects are considerably more temporally coherent, but there are flaws in more minor details (predicting fire in the middle of the mining clip below, for example):</p><div class="flex justify-center items-center"><img alt="500M" loading="lazy" width="1000" height="500" decoding="async" data-nimg="1" style="color:transparent" src="/1b.gif"/><img alt="500M" loading="lazy" width="1000" height="500" decoding="async" data-nimg="1" style="color:transparent" src="/1b-2.gif"/><img alt="500M" loading="lazy" width="1000" height="500" decoding="async" data-nimg="1" style="color:transparent" src="/1b-3.gif"/></div></div><h2 class="text-2xl font-bold text-center mt-8">Performance and Benchmarking</h2><div class="flex flex-col gap-4 px-0 md:px-16"><p>An important goal for this project was to stream model output in realtime, and in response to user input. Existing video diffusion models such as HeyGen, Pika and Runway each take 10-20 seconds to create one second of video. In order to match the experience of playing a game, however, the model must be able to generate video faster than realtime, with each frame taking 0.03 seconds or less to generate.</p><p>Part of this optimization effort can be done in software - Decart has pioneered a new AI inference stack written from scratch in CUDA, that far outpaces any open-source equivalent. With Decart’s optimizations, the model was able to run Nx faster on H100, unlocking real-time interactivity for the first time.</p><img alt="Bench" loading="lazy" width="1000" height="500" decoding="async" data-nimg="1" style="color:transparent" src="/bench.png"/><p>However, to get the remaining 10x improvement in speed and efficiency, hardware innovation is necessary. Due to its all-Transformer architecture, Oasis is able to run natively on Sohu, the Transformer ASIC chip from Etched. An 8xSohu node can run Oasis at Nfps, serving up to N users concurrently. This allows real time content to be streamed cheaply, unlocking new use cases that are impossible on GPUs.</p></div><h2 class="text-2xl font-bold text-center mt-8">The Future of Interactive Content</h2><div class="flex flex-col gap-4 px-0 md:px-16"><p>Oasis is an impressive technical demo, but we believe it points to an extremely exciting new generation of foundation models and consumer products. For example:</p><ul class="list-disc"><li><p>Creating and editing game content on-the-fly, even while playing, through prompting and latent editing</p></li><li><p>Generating content individually tailored for each user on social media</p></li><li><p>Real-time medical care, responding on the fly to patients in a video call</p></li><li><p>An AI teacher, who can generate videos to respond to students in a classroom.</p></li></ul><p>Etched and Decart are excited to partner to build this future together. Decart is on a mission to create new consumer experiences using their deep technical knowledge in AI. Etched is on a mission to deliver Sohu - a chip that will make Transformer inference orders of magnitude cheaper and faster. The integration of Oasis and its descendants between the two, from architecture to deployment, will ensure we can deliver this future faster than anyone else.</p></div></main><div class="flex flex-col gap-4 px-0 md:px-16 bg-black"><div class="flex flex-col gap-4 p-8 pb-20 gap-16 sm:p-20 md:px-48"><h2 class="text-2xl font-bold text-center text-white">Contributors</h2><p class="text-white text-center">Etched: <a class="underline" href="">Julian Quevedo</a>, etc, <a class="underline" href="">Spruce Campbell</a>, <a class="underline" href="">Quinn McIntyre</a>, <a class="underline" href="">Robert Wachen</a></p><p class="text-white text-center">Decart: <a class="underline" href="">Orian</a>, <a class="underline" href="">Metar</a>, <a class="underline" href="">Tamar</a>, <a class="underline" href="">Dean</a> etc</p></div></div></div><script src="/_next/static/chunks/webpack-0954c5d36ad957a9.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/a34f9d1faa5f3315-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/css/17e25edc667feb7e.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"3:I[2846,[],\"\"]\n5:I[5878,[\"310\",\"static/chunks/0e5ce63c-a42bf838a066fc6f.js\",\"401\",\"static/chunks/401-82f646aab98d244d.js\",\"931\",\"static/chunks/app/page-816450e1d70f9500.js\"],\"Image\"]\n6:I[6677,[\"310\",\"static/chunks/0e5ce63c-a42bf838a066fc6f.js\",\"401\",\"static/chunks/401-82f646aab98d244d.js\",\"931\",\"static/chunks/app/page-816450e1d70f9500.js\"],\"Carousel\"]\n7:I[6677,[\"310\",\"static/chunks/0e5ce63c-a42bf838a066fc6f.js\",\"401\",\"static/chunks/401-82f646aab98d244d.js\",\"931\",\"static/chunks/app/page-816450e1d70f9500.js\"],\"CarouselContent\"]\n8:I[6677,[\"310\",\"static/chunks/0e5ce63c-a42bf838a066fc6f.js\",\"401\",\"static/chunks/401-82f646aab98d244d.js\",\"931\",\"static/chunks/app/page-816450e1d70f9500.js\"],\"CarouselItem\"]\n9:I[6677,[\"310\",\"static/chunks/0e5ce63c-a42bf838a066fc6f.js\",\"401\",\"static/chunks/401-82f646aab98d244d.js\",\"931\",\"static/chunks/app/page-816450e1d70f9500.js\"],\"CarouselPrevious\"]\na:I[6677,[\"310\",\"static/chunks/0e5ce63c-a42bf838a066fc6f.js\",\"401\",\"static/chunks/401-82f646aab98d244d.js\",\"931\",\"static/chunks/app/page-816450e1d70f9500.js\"],\"CarouselNext\"]\nb:I[4707,[],\"\"]\nc:I[6423,[],\"\"]\ne:I[1060,[],\"\"]\nf:[]\n"])</script><script>self.__next_f.push([1,"0:[\"$\",\"$L3\",null,{\"buildId\":\"wfYPspDkhBMCQOVX_RmSM\",\"assetPrefix\":\"\",\"urlParts\":[\"\",\"\"],\"initialTree\":[\"\",{\"children\":[\"__PAGE__\",{}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"__PAGE__\",{},[[\"$L4\",[\"$\",\"div\",null,{\"className\":\"\",\"children\":[[\"$\",\"main\",null,{\"className\":\"flex flex-col gap-8  p-8 pb-20 gap-16 sm:p-20 md:px-48 \",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-col items-center gap-8\",\"children\":[[\"$\",\"h3\",null,{\"children\":\"October 29, 2024\"}],[\"$\",\"h1\",null,{\"className\":\"text-5xl font-bold text-center\",\"children\":\"Oasis: Generating Worlds in Realtime\"}],[\"$\",\"h2\",null,{\"className\":\"mt-[-20px] text-center mb-8\",\"children\":\"Etched Team, Decart Team\"}],[\"$\",\"$L5\",null,{\"className\":\"dark:invert\",\"src\":\"/image4.gif\",\"alt\":\"Next.js logo\",\"width\":600,\"height\":400,\"priority\":true}],[\"$\",\"div\",null,{\"className\":\"flex gap-4 items-center flex-col sm:flex-row\",\"children\":[[\"$\",\"a\",null,{\"className\":\"rounded-full border border-solid border-transparent transition-colors flex items-center justify-center bg-foreground text-background gap-2 hover:bg-[#383838] dark:hover:bg-[#ccc] text-sm sm:text-base h-10 sm:h-12 px-4 sm:px-5\",\"href\":\"\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":[[\"$\",\"$L5\",null,{\"className\":\"dark:invert\",\"src\":\"https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg\",\"alt\":\"GitHub\",\"width\":20,\"height\":20,\"style\":{\"filter\":\"invert(100%)\"}}],\"View code\"]}],[\"$\",\"a\",null,{\"className\":\"rounded-full border border-solid border-transparent transition-colors flex items-center justify-center bg-foreground text-background gap-2 hover:bg-[#383838] dark:hover:bg-[#ccc] text-sm sm:text-base h-10 sm:h-12 px-4 sm:px-5\",\"href\":\"\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"Play demo\"}],[\"$\",\"a\",null,{\"className\":\"rounded-full border border-solid border-black/[.08] dark:border-white/[.145] transition-colors flex items-center justify-center hover:bg-[#f2f2f2] dark:hover:bg-[#1a1a1a] hover:border-transparent text-sm sm:text-base text-gray-500 h-10 sm:h-12 px-4 sm:px-5 sm:min-w-44\",\"href\":\"https://nextjs.org/docs?utm_source=create-next-app\u0026utm_medium=appdir-template-tw\u0026utm_campaign=create-next-app\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"Paper (Coming Soon)\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"flex flex-col gap-4 px-0 lg:px-32\",\"children\":[[\"$\",\"p\",null,{\"className\":\"text-justify leading-relaxed\",\"children\":\"We have trained a new model, Oasis, that can generate 3D interactive worlds in realtime. In this technical report, we will discuss the model behavior and results, detail the architecture and training process, and release the code open-source. \"}],[\"$\",\"p\",null,{\"className\":\"text-justify leading-relaxed\",\"children\":\"Oasis is the first playable fully AI-based game engine. Streaming interactive AI-generated video to users is a technical challenge on par with developing the model itself, and in addition to our architecture, this report will detail the technical innovations that will be required to serve video models at scale, and our vision for the future of interactive content. \"}],[\"$\",\"div\",null,{\"className\":\"flex justify-center items-center mt-8\",\"children\":[\"$\",\"$L6\",null,{\"className\":\"w-full \",\"children\":[[\"$\",\"$L7\",null,{\"children\":[[\"$\",\"$L8\",\"0\",{\"className\":\"basis-1/3\",\"children\":[\"$\",\"div\",null,{\"className\":\"rounded-xl border bg-card text-card-foreground shadow\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex items-center justify-center p-0\",\"children\":[\"$\",\"$L5\",null,{\"src\":\"/image2.gif\",\"alt\":\"Carousel image 1\",\"width\":700,\"height\":300,\"className\":\"object-cover\"}]}]}]}],[\"$\",\"$L8\",\"1\",{\"className\":\"basis-1/3\",\"children\":[\"$\",\"div\",null,{\"className\":\"rounded-xl border bg-card text-card-foreground shadow\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex items-center justify-center p-0\",\"children\":[\"$\",\"$L5\",null,{\"src\":\"/image3.gif\",\"alt\":\"Carousel image 2\",\"width\":700,\"height\":300,\"className\":\"object-cover\"}]}]}]}],[\"$\",\"$L8\",\"2\",{\"className\":\"basis-1/3\",\"children\":[\"$\",\"div\",null,{\"className\":\"rounded-xl border bg-card text-card-foreground shadow\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex items-center justify-center p-0\",\"children\":[\"$\",\"$L5\",null,{\"src\":\"/image4.gif\",\"alt\":\"Carousel image 3\",\"width\":700,\"height\":300,\"className\":\"object-cover\"}]}]}]}]]}],[\"$\",\"$L9\",null,{}],[\"$\",\"$La\",null,{}]]}]}],[\"$\",\"h2\",null,{\"className\":\"text-2xl font-bold text-center mt-8\",\"children\":\"Architecture\"}],[\"$\",\"$L5\",null,{\"src\":\"/arch.png\",\"alt\":\"Architecture\",\"width\":1000,\"height\":500}]]}],[\"$\",\"div\",null,{\"className\":\"flex flex-col gap-4 px-0 md:px-16\",\"children\":[\"$\",\"p\",null,{\"children\":[\"The model is composed of two parts - an autoencoding patch tokenizer and a latent diffusion backbone. Notably, both are based on Transformer architectures - the autoencoder is based on \",[\"$\",\"a\",null,{\"className\":\"underline\",\"href\":\"https://arxiv.org/abs/2010.11929\",\"children\":\"ViT\"}],\" (Vision Transformer), and the backbone is based on \",[\"$\",\"a\",null,{\"className\":\"underline\",\"href\":\"https://arxiv.org/abs/2212.09748\",\"children\":\"DiT\"}],\" (Diffusion Transformer). Conventionally, image models (such as \",[\"$\",\"a\",null,{\"className\":\"underline\",\"href\":\"https://arxiv.org/abs/2112.10752\",\"children\":\"Stable Diffusion\"}],\") use VAEs and UNets, but we explicitly chose Transformers to ensure stable, predictable scaling laws, and fast inference on Sohu. \"]}]}],[\"$\",\"h3\",null,{\"className\":\"text-xl font-bold text-center mt-8\",\"children\":\"Tokenizer\"}],[\"$\",\"div\",null,{\"className\":\"flex flex-col gap-4 px-0 md:px-16\",\"children\":[[\"$\",\"p\",null,{\"children\":\"We experimented with novel approaches to tokenization to enable higher resolution and higher quality. We anticipated that more compute spent on patching would result in higher-quality output. To do this, we modified a Vision Transformer, reducing the dimensionality of the central layer and training it as a VAE. We enforce a dimensionality bottleneck and Gaussian distribution on the middle layer, using it to tokenize 10-by-10 pixel patches into latent tokens for the diffusion model. \"}],[\"$\",\"p\",null,{\"children\":\"We train our tokenizer on 1 frame of every second of our dataset, such that the model does not overfit to features in particular videos. We stop training once the validation quality gets worse and the model has overfit.\"}],[\"$\",\"$L5\",null,{\"src\":\"/tokenizer.png\",\"alt\":\"Tokenizer\",\"width\":1000,\"height\":500}],[\"$\",\"p\",null,{\"children\":\"We observe strong locality in the features in our tokenizers, namely that individual tokens often correspond directly to local regions in space. This allows us to reverse engineer feature attribution within the model. Once these features are understood, we can edit the model’s output simply by editing the latent token space. \"}],[\"$\",\"p\",null,{\"children\":\"An example here is selecting a blank item bar, and inserting a full toolbar in the menu via the latent representations:\"}],[\"$\",\"$L5\",null,{\"src\":\"/toolbar.png\",\"alt\":\"Toolbar\",\"width\":1000,\"height\":500}]]}],[\"$\",\"h3\",null,{\"className\":\"text-xl font-bold text-center mt-8\",\"children\":\"Diffusion Backbone\"}],[\"$\",\"div\",null,{\"className\":\"flex flex-col gap-4 px-0 md:px-16\",\"children\":[[\"$\",\"p\",null,{\"children\":[\"In contrast to bidirectional models such as Sora, Oasis can generate an unlimited length of video autoregressively, with the option for each frame to be conditioned on game input. This enables users to interact with the world in realtime. This behavior is based on the \",[\"$\",\"a\",null,{\"className\":\"underline\",\"href\":\"https://arxiv.org/abs/2408.00927\",\"children\":\"Diffusion Forcing\"}],\" paper, which proposed models that simultaneously predict future tokens while denoising past ones:\"]}],[\"$\",\"$L5\",null,{\"src\":\"/forcing.png\",\"alt\":\"Forcing\",\"width\":1000,\"height\":500}],[\"$\",\"p\",null,{\"children\":\"We replicate the Diffusion Forcing DiT backbone with some minor adjustments for speed and stability. In particular, we train 500M and 1B parameter checkpoints, for 200,000 steps, over one epoch of gameplay data.        \"}],[\"$\",\"p\",null,{\"children\":\"During development, we attempted to use an off-the-shelf LLaMA architecture to predict video frames from a discrete codebook, similar to the approach taken in LLaMA-Gen. While we found the model trained well and we were able to take advantage of the optimisation of many open-source libraries around LLaMA models, we were unable to get long-duration temporal consistency in the model’s outputs. For this reason, we chose to use a continuous architecture. We believe there are natural architectures for continuous data domains such as video and images, and that “vanilla” GPT models will not trivially generalize to this task.\"}]]}],[\"$\",\"h2\",null,{\"className\":\"text-2xl font-bold text-center mt-8\",\"children\":\"Results\"}],[\"$\",\"h3\",null,{\"className\":\"text-xl font-bold text-center mt-8\",\"children\":\"500M\"}],[\"$\",\"div\",null,{\"className\":\"flex flex-col gap-4 px-0 md:px-16\",\"children\":[[\"$\",\"p\",null,{\"children\":\"At the 500M parameter scale, there are recognizable features, but there is a high-degree of jitter, and the model struggles to remain coherent between frames:\"}],[\"$\",\"div\",null,{\"className\":\"flex justify-center items-center\",\"children\":[[\"$\",\"$L5\",null,{\"src\":\"/500m.gif\",\"alt\":\"500M\",\"width\":1000,\"height\":500}],[\"$\",\"$L5\",null,{\"src\":\"/500m-2.gif\",\"alt\":\"500M\",\"width\":1000,\"height\":500}],[\"$\",\"$L5\",null,{\"src\":\"/500m-3.gif\",\"alt\":\"500M\",\"width\":1000,\"height\":500}]]}]]}],[\"$\",\"h3\",null,{\"className\":\"text-xl font-bold text-center mt-8\",\"children\":\"1B\"}],[\"$\",\"div\",null,{\"className\":\"flex flex-col gap-4 px-0 md:px-16\",\"children\":[[\"$\",\"p\",null,{\"children\":\"At the 1B scale, the model begins to capture more complex behaviors and objects are considerably more temporally coherent, but there are flaws in more minor details (predicting fire in the middle of the mining clip below, for example):\"}],[\"$\",\"div\",null,{\"className\":\"flex justify-center items-center\",\"children\":[[\"$\",\"$L5\",null,{\"src\":\"/1b.gif\",\"alt\":\"500M\",\"width\":1000,\"height\":500}],[\"$\",\"$L5\",null,{\"src\":\"/1b-2.gif\",\"alt\":\"500M\",\"width\":1000,\"height\":500}],[\"$\",\"$L5\",null,{\"src\":\"/1b-3.gif\",\"alt\":\"500M\",\"width\":1000,\"height\":500}]]}]]}],[\"$\",\"h2\",null,{\"className\":\"text-2xl font-bold text-center mt-8\",\"children\":\"Performance and Benchmarking\"}],[\"$\",\"div\",null,{\"className\":\"flex flex-col gap-4 px-0 md:px-16\",\"children\":[[\"$\",\"p\",null,{\"children\":\"An important goal for this project was to stream model output in realtime, and in response to user input. Existing video diffusion models such as HeyGen, Pika and Runway each take 10-20 seconds to create one second of video. In order to match the experience of playing a game, however, the model must be able to generate video faster than realtime, with each frame taking 0.03 seconds or less to generate.\"}],[\"$\",\"p\",null,{\"children\":\"Part of this optimization effort can be done in software - Decart has pioneered a new AI inference stack written from scratch in CUDA, that far outpaces any open-source equivalent. With Decart’s optimizations, the model was able to run Nx faster on H100, unlocking real-time interactivity for the first time.\"}],[\"$\",\"$L5\",null,{\"src\":\"/bench.png\",\"alt\":\"Bench\",\"width\":1000,\"height\":500}],[\"$\",\"p\",null,{\"children\":\"However, to get the remaining 10x improvement in speed and efficiency, hardware innovation is necessary. Due to its all-Transformer architecture, Oasis is able to run natively on Sohu, the Transformer ASIC chip from Etched. An 8xSohu node can run Oasis at Nfps, serving up to N users concurrently. This allows real time content to be streamed cheaply, unlocking new use cases that are impossible on GPUs.\"}]]}],[\"$\",\"h2\",null,{\"className\":\"text-2xl font-bold text-center mt-8\",\"children\":\"The Future of Interactive Content\"}],[\"$\",\"div\",null,{\"className\":\"flex flex-col gap-4 px-0 md:px-16\",\"children\":[[\"$\",\"p\",null,{\"children\":\"Oasis is an impressive technical demo, but we believe it points to an extremely exciting new generation of foundation models and consumer products. For example:\"}],[\"$\",\"ul\",null,{\"className\":\"list-disc\",\"children\":[[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":\"Creating and editing game content on-the-fly, even while playing, through prompting and latent editing\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":\"Generating content individually tailored for each user on social media\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":\"Real-time medical care, responding on the fly to patients in a video call\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":\"An AI teacher, who can generate videos to respond to students in a classroom.\"}]}]]}],[\"$\",\"p\",null,{\"children\":\"Etched and Decart are excited to partner to build this future together. Decart is on a mission to create new consumer experiences using their deep technical knowledge in AI. Etched is on a mission to deliver Sohu - a chip that will make Transformer inference orders of magnitude cheaper and faster. The integration of Oasis and its descendants between the two, from architecture to deployment, will ensure we can deliver this future faster than anyone else.\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"flex flex-col gap-4 px-0 md:px-16 bg-black\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex flex-col gap-4 p-8 pb-20 gap-16 sm:p-20 md:px-48\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-2xl font-bold text-center text-white\",\"children\":\"Contributors\"}],[\"$\",\"p\",null,{\"className\":\"text-white text-center\",\"children\":[\"Etched: \",[\"$\",\"a\",null,{\"className\":\"underline\",\"href\":\"\",\"children\":\"Julian Quevedo\"}],\", etc, \",[\"$\",\"a\",null,{\"className\":\"underline\",\"href\":\"\",\"children\":\"Spruce Campbell\"}],\", \",[\"$\",\"a\",null,{\"className\":\"underline\",\"href\":\"\",\"children\":\"Quinn McIntyre\"}],\", \",[\"$\",\"a\",null,{\"className\":\"underline\",\"href\":\"\",\"children\":\"Robert Wachen\"}]]}],[\"$\",\"p\",null,{\"className\":\"text-white text-center\",\"children\":[\"Decart: \",[\"$\",\"a\",null,{\"className\":\"underline\",\"href\":\"\",\"children\":\"Orian\"}],\", \",[\"$\",\"a\",null,{\"className\":\"underline\",\"href\":\"\",\"children\":\"Metar\"}],\", \",[\"$\",\"a\",null,{\"className\":\"underline\",\"href\":\"\",\"children\":\"Tamar\"}],\", \",[\"$\",\"a\",null,{\"className\":\"underline\",\"href\":\"\",\"children\":\"Dean\"}],\" etc\"]}]]}]}]]}],null],null],null]},[[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/17e25edc667feb7e.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"__className_d65c78 antialiased\",\"children\":[\"$\",\"$Lb\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$Lc\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[]}]}]}]],null],null],\"couldBeIntercepted\":false,\"initialHead\":[null,\"$Ld\"],\"globalErrorComponent\":\"$e\",\"missingSlots\":\"$Wf\"}]\n"])</script><script>self.__next_f.push([1,"d:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"Oasis\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"Generating Worlds in Realtime\"}],[\"$\",\"link\",\"4\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"16x16\"}],[\"$\",\"meta\",\"5\",{\"name\":\"next-size-adjust\"}]]\n4:null\n"])</script></body></html>