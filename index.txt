2:I[5878,["310","static/chunks/0e5ce63c-a42bf838a066fc6f.js","401","static/chunks/401-82f646aab98d244d.js","931","static/chunks/app/page-816450e1d70f9500.js"],"Image"]
3:I[6677,["310","static/chunks/0e5ce63c-a42bf838a066fc6f.js","401","static/chunks/401-82f646aab98d244d.js","931","static/chunks/app/page-816450e1d70f9500.js"],"Carousel"]
4:I[6677,["310","static/chunks/0e5ce63c-a42bf838a066fc6f.js","401","static/chunks/401-82f646aab98d244d.js","931","static/chunks/app/page-816450e1d70f9500.js"],"CarouselContent"]
5:I[6677,["310","static/chunks/0e5ce63c-a42bf838a066fc6f.js","401","static/chunks/401-82f646aab98d244d.js","931","static/chunks/app/page-816450e1d70f9500.js"],"CarouselItem"]
6:I[6677,["310","static/chunks/0e5ce63c-a42bf838a066fc6f.js","401","static/chunks/401-82f646aab98d244d.js","931","static/chunks/app/page-816450e1d70f9500.js"],"CarouselPrevious"]
7:I[6677,["310","static/chunks/0e5ce63c-a42bf838a066fc6f.js","401","static/chunks/401-82f646aab98d244d.js","931","static/chunks/app/page-816450e1d70f9500.js"],"CarouselNext"]
8:I[4707,[],""]
9:I[6423,[],""]
0:["wfYPspDkhBMCQOVX_RmSM",[[["",{"children":["__PAGE__",{}]},"$undefined","$undefined",true],["",{"children":["__PAGE__",{},[["$L1",["$","div",null,{"className":"","children":[["$","main",null,{"className":"flex flex-col gap-8  p-8 pb-20 gap-16 sm:p-20 md:px-48 ","children":[["$","div",null,{"className":"flex flex-col items-center gap-8","children":[["$","h3",null,{"children":"October 29, 2024"}],["$","h1",null,{"className":"text-5xl font-bold text-center","children":"Oasis: Generating Worlds in Realtime"}],["$","h2",null,{"className":"mt-[-20px] text-center mb-8","children":"Etched Team, Decart Team"}],["$","$L2",null,{"className":"dark:invert","src":"/image4.gif","alt":"Next.js logo","width":600,"height":400,"priority":true}],["$","div",null,{"className":"flex gap-4 items-center flex-col sm:flex-row","children":[["$","a",null,{"className":"rounded-full border border-solid border-transparent transition-colors flex items-center justify-center bg-foreground text-background gap-2 hover:bg-[#383838] dark:hover:bg-[#ccc] text-sm sm:text-base h-10 sm:h-12 px-4 sm:px-5","href":"","target":"_blank","rel":"noopener noreferrer","children":[["$","$L2",null,{"className":"dark:invert","src":"https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg","alt":"GitHub","width":20,"height":20,"style":{"filter":"invert(100%)"}}],"View code"]}],["$","a",null,{"className":"rounded-full border border-solid border-transparent transition-colors flex items-center justify-center bg-foreground text-background gap-2 hover:bg-[#383838] dark:hover:bg-[#ccc] text-sm sm:text-base h-10 sm:h-12 px-4 sm:px-5","href":"","target":"_blank","rel":"noopener noreferrer","children":"Play demo"}],["$","a",null,{"className":"rounded-full border border-solid border-black/[.08] dark:border-white/[.145] transition-colors flex items-center justify-center hover:bg-[#f2f2f2] dark:hover:bg-[#1a1a1a] hover:border-transparent text-sm sm:text-base text-gray-500 h-10 sm:h-12 px-4 sm:px-5 sm:min-w-44","href":"https://nextjs.org/docs?utm_source=create-next-app&utm_medium=appdir-template-tw&utm_campaign=create-next-app","target":"_blank","rel":"noopener noreferrer","children":"Paper (Coming Soon)"}]]}]]}],["$","div",null,{"className":"flex flex-col gap-4 px-0 lg:px-32","children":[["$","p",null,{"className":"text-justify leading-relaxed","children":"We have trained a new model, Oasis, that can generate 3D interactive worlds in realtime. In this technical report, we will discuss the model behavior and results, detail the architecture and training process, and release the code open-source. "}],["$","p",null,{"className":"text-justify leading-relaxed","children":"Oasis is the first playable fully AI-based game engine. Streaming interactive AI-generated video to users is a technical challenge on par with developing the model itself, and in addition to our architecture, this report will detail the technical innovations that will be required to serve video models at scale, and our vision for the future of interactive content. "}],["$","div",null,{"className":"flex justify-center items-center mt-8","children":["$","$L3",null,{"className":"w-full ","children":[["$","$L4",null,{"children":[["$","$L5","0",{"className":"basis-1/3","children":["$","div",null,{"className":"rounded-xl border bg-card text-card-foreground shadow","children":["$","div",null,{"className":"flex items-center justify-center p-0","children":["$","$L2",null,{"src":"/image2.gif","alt":"Carousel image 1","width":700,"height":300,"className":"object-cover"}]}]}]}],["$","$L5","1",{"className":"basis-1/3","children":["$","div",null,{"className":"rounded-xl border bg-card text-card-foreground shadow","children":["$","div",null,{"className":"flex items-center justify-center p-0","children":["$","$L2",null,{"src":"/image3.gif","alt":"Carousel image 2","width":700,"height":300,"className":"object-cover"}]}]}]}],["$","$L5","2",{"className":"basis-1/3","children":["$","div",null,{"className":"rounded-xl border bg-card text-card-foreground shadow","children":["$","div",null,{"className":"flex items-center justify-center p-0","children":["$","$L2",null,{"src":"/image4.gif","alt":"Carousel image 3","width":700,"height":300,"className":"object-cover"}]}]}]}]]}],["$","$L6",null,{}],["$","$L7",null,{}]]}]}],["$","h2",null,{"className":"text-2xl font-bold text-center mt-8","children":"Architecture"}],["$","$L2",null,{"src":"/arch.png","alt":"Architecture","width":1000,"height":500}]]}],["$","div",null,{"className":"flex flex-col gap-4 px-0 md:px-16","children":["$","p",null,{"children":["The model is composed of two parts - an autoencoding patch tokenizer and a latent diffusion backbone. Notably, both are based on Transformer architectures - the autoencoder is based on ",["$","a",null,{"className":"underline","href":"https://arxiv.org/abs/2010.11929","children":"ViT"}]," (Vision Transformer), and the backbone is based on ",["$","a",null,{"className":"underline","href":"https://arxiv.org/abs/2212.09748","children":"DiT"}]," (Diffusion Transformer). Conventionally, image models (such as ",["$","a",null,{"className":"underline","href":"https://arxiv.org/abs/2112.10752","children":"Stable Diffusion"}],") use VAEs and UNets, but we explicitly chose Transformers to ensure stable, predictable scaling laws, and fast inference on Sohu. "]}]}],["$","h3",null,{"className":"text-xl font-bold text-center mt-8","children":"Tokenizer"}],["$","div",null,{"className":"flex flex-col gap-4 px-0 md:px-16","children":[["$","p",null,{"children":"We experimented with novel approaches to tokenization to enable higher resolution and higher quality. We anticipated that more compute spent on patching would result in higher-quality output. To do this, we modified a Vision Transformer, reducing the dimensionality of the central layer and training it as a VAE. We enforce a dimensionality bottleneck and Gaussian distribution on the middle layer, using it to tokenize 10-by-10 pixel patches into latent tokens for the diffusion model. "}],["$","p",null,{"children":"We train our tokenizer on 1 frame of every second of our dataset, such that the model does not overfit to features in particular videos. We stop training once the validation quality gets worse and the model has overfit."}],["$","$L2",null,{"src":"/tokenizer.png","alt":"Tokenizer","width":1000,"height":500}],["$","p",null,{"children":"We observe strong locality in the features in our tokenizers, namely that individual tokens often correspond directly to local regions in space. This allows us to reverse engineer feature attribution within the model. Once these features are understood, we can edit the model’s output simply by editing the latent token space. "}],["$","p",null,{"children":"An example here is selecting a blank item bar, and inserting a full toolbar in the menu via the latent representations:"}],["$","$L2",null,{"src":"/toolbar.png","alt":"Toolbar","width":1000,"height":500}]]}],["$","h3",null,{"className":"text-xl font-bold text-center mt-8","children":"Diffusion Backbone"}],["$","div",null,{"className":"flex flex-col gap-4 px-0 md:px-16","children":[["$","p",null,{"children":["In contrast to bidirectional models such as Sora, Oasis can generate an unlimited length of video autoregressively, with the option for each frame to be conditioned on game input. This enables users to interact with the world in realtime. This behavior is based on the ",["$","a",null,{"className":"underline","href":"https://arxiv.org/abs/2408.00927","children":"Diffusion Forcing"}]," paper, which proposed models that simultaneously predict future tokens while denoising past ones:"]}],["$","$L2",null,{"src":"/forcing.png","alt":"Forcing","width":1000,"height":500}],["$","p",null,{"children":"We replicate the Diffusion Forcing DiT backbone with some minor adjustments for speed and stability. In particular, we train 500M and 1B parameter checkpoints, for 200,000 steps, over one epoch of gameplay data.        "}],["$","p",null,{"children":"During development, we attempted to use an off-the-shelf LLaMA architecture to predict video frames from a discrete codebook, similar to the approach taken in LLaMA-Gen. While we found the model trained well and we were able to take advantage of the optimisation of many open-source libraries around LLaMA models, we were unable to get long-duration temporal consistency in the model’s outputs. For this reason, we chose to use a continuous architecture. We believe there are natural architectures for continuous data domains such as video and images, and that “vanilla” GPT models will not trivially generalize to this task."}]]}],["$","h2",null,{"className":"text-2xl font-bold text-center mt-8","children":"Results"}],["$","h3",null,{"className":"text-xl font-bold text-center mt-8","children":"500M"}],["$","div",null,{"className":"flex flex-col gap-4 px-0 md:px-16","children":[["$","p",null,{"children":"At the 500M parameter scale, there are recognizable features, but there is a high-degree of jitter, and the model struggles to remain coherent between frames:"}],["$","div",null,{"className":"flex justify-center items-center","children":[["$","$L2",null,{"src":"/500m.gif","alt":"500M","width":1000,"height":500}],["$","$L2",null,{"src":"/500m-2.gif","alt":"500M","width":1000,"height":500}],["$","$L2",null,{"src":"/500m-3.gif","alt":"500M","width":1000,"height":500}]]}]]}],["$","h3",null,{"className":"text-xl font-bold text-center mt-8","children":"1B"}],["$","div",null,{"className":"flex flex-col gap-4 px-0 md:px-16","children":[["$","p",null,{"children":"At the 1B scale, the model begins to capture more complex behaviors and objects are considerably more temporally coherent, but there are flaws in more minor details (predicting fire in the middle of the mining clip below, for example):"}],["$","div",null,{"className":"flex justify-center items-center","children":[["$","$L2",null,{"src":"/1b.gif","alt":"500M","width":1000,"height":500}],["$","$L2",null,{"src":"/1b-2.gif","alt":"500M","width":1000,"height":500}],["$","$L2",null,{"src":"/1b-3.gif","alt":"500M","width":1000,"height":500}]]}]]}],["$","h2",null,{"className":"text-2xl font-bold text-center mt-8","children":"Performance and Benchmarking"}],["$","div",null,{"className":"flex flex-col gap-4 px-0 md:px-16","children":[["$","p",null,{"children":"An important goal for this project was to stream model output in realtime, and in response to user input. Existing video diffusion models such as HeyGen, Pika and Runway each take 10-20 seconds to create one second of video. In order to match the experience of playing a game, however, the model must be able to generate video faster than realtime, with each frame taking 0.03 seconds or less to generate."}],["$","p",null,{"children":"Part of this optimization effort can be done in software - Decart has pioneered a new AI inference stack written from scratch in CUDA, that far outpaces any open-source equivalent. With Decart’s optimizations, the model was able to run Nx faster on H100, unlocking real-time interactivity for the first time."}],["$","$L2",null,{"src":"/bench.png","alt":"Bench","width":1000,"height":500}],["$","p",null,{"children":"However, to get the remaining 10x improvement in speed and efficiency, hardware innovation is necessary. Due to its all-Transformer architecture, Oasis is able to run natively on Sohu, the Transformer ASIC chip from Etched. An 8xSohu node can run Oasis at Nfps, serving up to N users concurrently. This allows real time content to be streamed cheaply, unlocking new use cases that are impossible on GPUs."}]]}],["$","h2",null,{"className":"text-2xl font-bold text-center mt-8","children":"The Future of Interactive Content"}],["$","div",null,{"className":"flex flex-col gap-4 px-0 md:px-16","children":[["$","p",null,{"children":"Oasis is an impressive technical demo, but we believe it points to an extremely exciting new generation of foundation models and consumer products. For example:"}],["$","ul",null,{"className":"list-disc","children":[["$","li",null,{"children":["$","p",null,{"children":"Creating and editing game content on-the-fly, even while playing, through prompting and latent editing"}]}],["$","li",null,{"children":["$","p",null,{"children":"Generating content individually tailored for each user on social media"}]}],["$","li",null,{"children":["$","p",null,{"children":"Real-time medical care, responding on the fly to patients in a video call"}]}],["$","li",null,{"children":["$","p",null,{"children":"An AI teacher, who can generate videos to respond to students in a classroom."}]}]]}],["$","p",null,{"children":"Etched and Decart are excited to partner to build this future together. Decart is on a mission to create new consumer experiences using their deep technical knowledge in AI. Etched is on a mission to deliver Sohu - a chip that will make Transformer inference orders of magnitude cheaper and faster. The integration of Oasis and its descendants between the two, from architecture to deployment, will ensure we can deliver this future faster than anyone else."}]]}]]}],["$","div",null,{"className":"flex flex-col gap-4 px-0 md:px-16 bg-black","children":["$","div",null,{"className":"flex flex-col gap-4 p-8 pb-20 gap-16 sm:p-20 md:px-48","children":[["$","h2",null,{"className":"text-2xl font-bold text-center text-white","children":"Contributors"}],["$","p",null,{"className":"text-white text-center","children":["Etched: ",["$","a",null,{"className":"underline","href":"","children":"Julian Quevedo"}],", etc, ",["$","a",null,{"className":"underline","href":"","children":"Spruce Campbell"}],", ",["$","a",null,{"className":"underline","href":"","children":"Quinn McIntyre"}],", ",["$","a",null,{"className":"underline","href":"","children":"Robert Wachen"}]]}],["$","p",null,{"className":"text-white text-center","children":["Decart: ",["$","a",null,{"className":"underline","href":"","children":"Orian"}],", ",["$","a",null,{"className":"underline","href":"","children":"Metar"}],", ",["$","a",null,{"className":"underline","href":"","children":"Tamar"}],", ",["$","a",null,{"className":"underline","href":"","children":"Dean"}]," etc"]}]]}]}]]}],null],null],null]},[[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/17e25edc667feb7e.css","precedence":"next","crossOrigin":"$undefined"}]],["$","html",null,{"lang":"en","children":["$","body",null,{"className":"__className_d65c78 antialiased","children":["$","$L8",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L9",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":"404"}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],"notFoundStyles":[]}]}]}]],null],null],["$La",null]]]]
a:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"Oasis"}],["$","meta","3",{"name":"description","content":"Generating Worlds in Realtime"}],["$","link","4",{"rel":"icon","href":"/favicon.ico","type":"image/x-icon","sizes":"16x16"}],["$","meta","5",{"name":"next-size-adjust"}]]
1:null
